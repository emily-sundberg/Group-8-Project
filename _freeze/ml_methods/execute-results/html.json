{
  "hash": "cddf1b454d9cd1b4ac62de4e2672f83c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ML Methods\"\nformat:\n  html:\n    code-fold: true\n    code-overflow: wrap\n    code-summary: \"Show code\"\n    code-tools: true\n\nexecute:\n  echo: true\n  eval: false\n  output: true\n  freeze: auto\n---\n\n::: {#setup-topic-2-2 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set Plotly theme\npio.templates.default = \"plotly_white\"\n\nprint(\"‚úì All libraries loaded successfully!\")\n```\n:::\n\n\n## Data Loading and Exploration\n\n::: {#load-data-topic-2-2 .cell execution_count=2}\n``` {.python .cell-code}\n# Load lightcast job postings data\ndf = pd.read_csv('data/lightcast_job_postings.csv')\n\nprint(f\"Dataset Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\nprint(f\"\\nFirst few rows:\")\ndf.head()\n```\n:::\n\n\n::: {#data-inspection .cell execution_count=3}\n``` {.python .cell-code}\n# Check data quality\nprint(\"=\"*80)\nprint(\"DATA QUALITY ASSESSMENT\")\nprint(\"=\"*80)\n\n# Key columns for analysis\nkey_columns = ['SALARY', 'STATE', 'TITLE', 'NAICS_2022_2', 'SOC_2', 'ONET', 'LIGHTCAST_SECTORS']\n\ninfo_df = pd.DataFrame({\n    'Column': key_columns,\n    'Missing': [df[col].isnull().sum() if col in df.columns else 'N/A' for col in key_columns],\n    'Missing %': [f\"{(df[col].isnull().sum() / len(df) * 100):.2f}%\" if col in df.columns else 'N/A' for col in key_columns],\n    'Unique Values': [df[col].nunique() if col in df.columns else 'N/A' for col in key_columns]\n})\n\nprint(info_df.to_string(index=False))\n\n# Salary statistics\nif 'SALARY' in df.columns:\n    print(f\"\\nSalary Statistics:\")\n    print(f\"  Mean: ${df['SALARY'].mean():,.2f}\")\n    print(f\"  Median: ${df['SALARY'].median():,.2f}\")\n    print(f\"  Std Dev: ${df['SALARY'].std():,.2f}\")\n    print(f\"  Range: ${df['SALARY'].min():,.2f} - ${df['SALARY'].max():,.2f}\")\n```\n:::\n\n\n## Data Preprocessing\n\n::: {#preprocessing .cell execution_count=4}\n``` {.python .cell-code}\n# Create working copy\ndf_clean = df.copy()\n\n# Remove rows with missing salary or state\nrequired_cols = ['SALARY', 'STATE']\ninitial_rows = len(df_clean)\ndf_clean = df_clean.dropna(subset=required_cols)\nremoved_rows = initial_rows - len(df_clean)\n\nprint(f\"Removed {removed_rows:,} rows with missing salary or state data\")\nprint(f\"Final dataset: {len(df_clean):,} rows\")\n```\n:::\n\n\n::: {#b9404a4c .cell execution_count=5}\n``` {.python .cell-code}\n## Political Leaning Classification (Fixed)\n\n#| label: fix-fips-and-political-leaning\n#| code-fold: true\n\n# FIPS code to state abbreviation mapping\nfips_to_state = {\n    1: 'AL', 2: 'AK', 4: 'AZ', 5: 'AR', 6: 'CA', 8: 'CO', 9: 'CT', 10: 'DE',\n    11: 'DC', 12: 'FL', 13: 'GA', 15: 'HI', 16: 'ID', 17: 'IL', 18: 'IN',\n    19: 'IA', 20: 'KS', 21: 'KY', 22: 'LA', 23: 'ME', 24: 'MD', 25: 'MA',\n    26: 'MI', 27: 'MN', 28: 'MS', 29: 'MO', 30: 'MT', 31: 'NE', 32: 'NV',\n    33: 'NH', 34: 'NJ', 35: 'NM', 36: 'NY', 37: 'NC', 38: 'ND', 39: 'OH',\n    40: 'OK', 41: 'OR', 42: 'PA', 44: 'RI', 45: 'SC', 46: 'SD', 47: 'TN',\n    48: 'TX', 49: 'UT', 50: 'VT', 51: 'VA', 53: 'WA', 54: 'WV', 55: 'WI',\n    56: 'WY', 72: 'PR'\n}\n\n# Convert FIPS codes to state abbreviations\ndf_clean['STATE_ABBREV'] = df_clean['STATE'].apply(\n    lambda x: fips_to_state.get(int(x), 'Unknown') if pd.notna(x) else 'Unknown'\n)\n\n# Political classifications\nred_states = ['AL', 'AK', 'AR', 'FL', 'ID', 'IN', 'IA', 'KS', 'KY', \n              'LA', 'MS', 'MO', 'MT', 'NE', 'ND', 'OH', 'OK', 'SC', \n              'SD', 'TN', 'TX', 'UT', 'WV', 'WY']\n\nblue_states = ['CA', 'CO', 'CT', 'DE', 'HI', 'IL', 'ME', 'MD', 'MA', \n               'MI', 'MN', 'NH', 'NJ', 'NM', 'NY', 'OR', 'PA', 'RI', \n               'VT', 'VA', 'WA', 'WI', 'DC']\n\nswing_states = ['AZ', 'GA', 'NC', 'NV']\n\ndef assign_political_leaning(state_abbrev):\n    if pd.isna(state_abbrev) or state_abbrev == 'Unknown':\n        return 'Unknown'\n    state_abbrev = str(state_abbrev).upper()\n    if state_abbrev in red_states:\n        return 'Red'\n    elif state_abbrev in blue_states:\n        return 'Blue'\n    elif state_abbrev in swing_states:\n        return 'Swing'\n    else:\n        return 'Other'\n\ndf_clean['political_leaning'] = df_clean['STATE_ABBREV'].apply(assign_political_leaning)\n\nprint(\"=\"*60)\nprint(\"POLITICAL LEANING DISTRIBUTION (FIXED)\")\nprint(\"=\"*60)\nprint(df_clean['political_leaning'].value_counts())\nprint(\"\\nPercentage:\")\nprint((df_clean['political_leaning'].value_counts() / len(df_clean) * 100).round(2))\n```\n:::\n\n\n::: {#visualize-political-distribution .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\n# Visualize political leaning distribution\nfig = px.pie(\n    values=df_clean['political_leaning'].value_counts().values,\n    names=df_clean['political_leaning'].value_counts().index,\n    title='Distribution of Jobs by Political Leaning of State',\n    hole=0.4,\n    color_discrete_map={'Red': '#FF6B6B', 'Blue': '#4ECDC4', 'Swing': '#FFD93D', 'Other': '#95A5A6'}\n)\nfig.update_layout(template=\"plotly_white\", height=400)\nfig.show()\n\nfig.write_html(\"./output/visualize_political_distribution.html\")\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/visualize_political_distribution.html\" title=\"Distribution of Jobs by Political Leaning of State\"></iframe>\n```\n\n## K-Means Clustering (unsupervised)\n\n### Setup and Feature Engineering\n\n::: {#clustering-reference-label .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Determine which reference label to use (SOC, NAICS, or ONET)\nreference_label = None\nfor label in ['SOC_2', 'NAICS_2022_2', 'ONET', 'LIGHTCAST_SECTORS']:\n    if label in df_clean.columns and df_clean[label].notna().sum() > 0:\n        reference_label = label\n        print(f\"‚úì Using {label} as reference label\")\n        break\n\nif reference_label is None:\n    print(\"No classification column found. Using TITLE as reference.\")\n    reference_label = 'TITLE'\n\nprint(f\"\\nReference Label: {reference_label}\")\nprint(f\"Unique values: {df_clean[reference_label].nunique():,}\")\nprint(f\"\\nTop 10 {reference_label} categories:\")\nprint(df_clean[reference_label].value_counts().head(10))\n```\n:::\n\n\n::: {#clustering-feature-engineering .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"View feature engineering code\"}\n# Prepare features for clustering\ndf_cluster = df_clean.copy()\n\nprint(\"=\"*80)\nprint(\"FEATURE ENGINEERING FOR CLUSTERING\")\nprint(\"=\"*80)\n\n# Encode categorical variables\nencoders = {}\ncategorical_cols = ['STATE', 'TITLE', 'political_leaning', 'LIGHTCAST_SECTORS']\n\nprint(f\"\\nEncoding categorical variables:\")\nfor col in categorical_cols:\n    if col in df_cluster.columns:\n        le = LabelEncoder()\n        df_cluster[f'{col}_encoded'] = le.fit_transform(\n            df_cluster[col].fillna('Unknown').astype(str)\n        )\n        encoders[col] = le\n        print(f\"  ‚úì {col}: {df_cluster[col].nunique()} unique values\")\n\n# Select clustering features\nclustering_features = ['SALARY']\n\nfor col in categorical_cols:\n    encoded_col = f'{col}_encoded'\n    if encoded_col in df_cluster.columns:\n        clustering_features.append(encoded_col)\n\n# Add years of experience if available\nif 'MIN_YEARS_EXPERIENCE' in df_cluster.columns:\n    df_cluster['MIN_YEARS_EXPERIENCE'] = pd.to_numeric(\n        df_cluster['MIN_YEARS_EXPERIENCE'], errors='coerce'\n    )\n    clustering_features.append('MIN_YEARS_EXPERIENCE')\n    print(f\"  ‚úì MIN_YEARS_EXPERIENCE: numeric feature\")\n\nprint(f\"\\nüìä Total Clustering Features: {len(clustering_features)}\")\nprint(\"\\nFeature List:\")\nfor i, feature in enumerate(clustering_features, 1):\n    print(f\"  {i}. {feature}\")\n\n# Prepare feature matrix\nX_cluster = df_cluster[clustering_features].fillna(df_cluster[clustering_features].mean())\nprint(f\"\\nFeature Matrix Shape: {X_cluster.shape}\")\n```\n:::\n\n\n### Determine Optimal K\n\n::: {#determine-optimal-clusters .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"View elbow method code\"}\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\n# Test different numbers of clusters\nK_range = range(2, 11)\ninertias = []\nsilhouette_scores = []\n\nprint(\"Testing different numbers of clusters...\")\nprint(f\"{'k':<5} {'Inertia':<15} {'Silhouette Score'}\")\nprint(\"-\" * 40)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n    silhouette_scores.append(sil_score)\n    print(f\"{k:<5} {kmeans.inertia_:<15.2f} {sil_score:.4f}\")\n\noptimal_k = list(K_range)[silhouette_scores.index(max(silhouette_scores))]\nprint(f\"\\nüí° Optimal k based on Silhouette Score: {optimal_k}\")\n```\n:::\n\n\n::: {#plot-elbow-silhouette .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Visualize elbow curve and silhouette scores\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=('Elbow Method', 'Silhouette Score Method')\n)\n\nfig.add_trace(\n    go.Scatter(x=list(K_range), y=inertias, mode='lines+markers', \n               name='Inertia', line=dict(color='blue')),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=list(K_range), y=silhouette_scores, mode='lines+markers', \n               name='Silhouette', line=dict(color='orange')),\n    row=1, col=2\n)\n\nfig.update_xaxes(title_text=\"Number of Clusters (k)\", row=1, col=1)\nfig.update_xaxes(title_text=\"Number of Clusters (k)\", row=1, col=2)\nfig.update_yaxes(title_text=\"Inertia\", row=1, col=1)\nfig.update_yaxes(title_text=\"Silhouette Score\", row=1, col=2)\n\nfig.update_layout(\n    height=400, \n    showlegend=False, \n    template=\"plotly_white\",\n    title_text=\"Determining Optimal Number of Clusters\"\n)\nfig.show()\n\nfig.write_html(\"./output/elbow_curve.html\")\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/elbow_curve.html\" title=\"Elbow Curve and Silhouette Score\"></iframe>\n```\n\n\n### Cluster Analysis\n\n::: {#perform-kmeans .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# Perform KMeans with k=5 (per assignment requirements)\nn_clusters = 5\n\nprint(f\"Performing KMeans with k={n_clusters} clusters...\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\ndf_cluster['cluster'] = kmeans.fit_predict(X_scaled)\n\nprint(f\"‚úì Clustering complete!\")\nprint(f\"\\nCluster Distribution:\")\ncluster_counts = df_cluster['cluster'].value_counts().sort_index()\nfor cluster_id, count in cluster_counts.items():\n    pct = (count / len(df_cluster)) * 100\n    print(f\"  Cluster {cluster_id}: {count:,} jobs ({pct:.1f}%)\")\n```\n:::\n\n\n::: {#visualize-clusters .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\n# Sample for performance (5000 points)\nsample_size = min(5000, len(df_cluster))\ndf_sample = df_cluster.sample(sample_size, random_state=42)\n\nfig = px.scatter(\n    df_sample,\n    x='SALARY',\n    y='STATE_encoded',\n    color='cluster',\n    hover_data=['TITLE', 'political_leaning', reference_label] if 'TITLE' in df_sample.columns else None,\n    title=f'KMeans Clustering Results (k={n_clusters}, n={sample_size:,} sample)',\n    labels={'SALARY': 'Annual Salary ($)', 'STATE_encoded': 'State (Encoded)', 'cluster': 'Cluster'},\n    color_continuous_scale='Viridis'\n)\nfig.update_layout(template=\"plotly_white\", height=550, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\nfig.write_html(\"./output/clusters.html\")\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/clusters.html\" title=\"Clusters\"></iframe>\n```\n\n\n\n**Key Findings:**\n- Cluster 0: Entry-level positions ($50-100k)\n- Cluster 4: Senior roles ($200k+)\n- Geographic clustering evident by state encoding\n\n::: {#cluster-profiling .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\n# Analyze cluster characteristics\nprint(\"=\"*80)\nprint(\"CLUSTER PROFILES\")\nprint(\"=\"*80)\n\ncluster_profiles = df_cluster.groupby('cluster').agg({\n    'SALARY': ['mean', 'median', 'std', 'min', 'max'],\n    'political_leaning': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Mixed',\n    'cluster': 'count'\n})\n\ncluster_profiles.columns = [\n    'Avg_Salary', 'Median_Salary', 'Salary_StdDev', 'Min_Salary', 'Max_Salary',\n    'Dominant_Political', 'Count'\n]\n\ncluster_profiles = cluster_profiles.round(2)\nprint(cluster_profiles)\n```\n:::\n\n\n::: {#visualize-cluster-profiles .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\n# Visualize cluster salary profiles\nprofile_df = cluster_profiles.reset_index()\n\nfig = px.bar(\n    profile_df,\n    x='cluster',\n    y='Avg_Salary',\n    text='Count',\n    title='Average Salary by Cluster',\n    labels={'cluster': 'Cluster', 'Avg_Salary': 'Average Salary ($)'},\n    color='Avg_Salary',\n    color_continuous_scale='Viridis'\n)\nfig.update_traces(texttemplate='n=%{text:,}', textposition='outside')\nfig.update_layout(template=\"plotly_white\", height=450, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\nfig.write_html(\"./output/cluster_profiles.html\")\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/cluster_profiles.html\" title=\"Cluster Profiles\"></iframe>\n```\n\n::: {#cluster-reference-comparison .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\n# Compare clusters with reference labels\nprint(f\"\\n{'='*80}\")\nprint(f\"CLUSTER vs {reference_label.upper()} COMPARISON\")\nprint(f\"{'='*80}\")\n\nfor cluster_id in sorted(df_cluster['cluster'].unique()):\n    cluster_data = df_cluster[df_cluster['cluster'] == cluster_id]\n    top_categories = cluster_data[reference_label].value_counts().head(5)\n    \n    print(f\"\\nüìä Cluster {cluster_id}:\")\n    print(f\"   Size: {len(cluster_data):,} jobs\")\n    print(f\"   Avg Salary: ${cluster_data['SALARY'].mean():,.2f}\")\n    if 'political_leaning' in cluster_data.columns:\n        print(f\"   Dominant Political: {cluster_data['political_leaning'].mode()[0] if len(cluster_data['political_leaning'].mode()) > 0 else 'Mixed'}\")\n    print(f\"   Top 5 {reference_label} categories:\")\n    for category, count in top_categories.items():\n        pct = (count / len(cluster_data)) * 100\n        print(f\"     ‚Ä¢ {category}: {count:,} ({pct:.1f}%)\")\n```\n:::\n\n\n## Multiple Regression Analysis\n\n::: {#regression-feature-engineering .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"View feature engineering code\"}\n# Prepare features for salary prediction\ndf_reg = df_clean.copy()\n\nprint(\"=\"*80)\nprint(\"FEATURE ENGINEERING FOR SALARY PREDICTION\")\nprint(\"=\"*80)\n\n# Encode categorical variables\nle_reg = {}\ncategorical_features = ['STATE', 'TITLE', 'political_leaning', 'LIGHTCAST_SECTORS']\n\n# Add SOC or NAICS if available\nif 'SOC_2' in df_reg.columns:\n    categorical_features.append('SOC_2')\nelif 'NAICS_2022_2' in df_reg.columns:\n    categorical_features.append('NAICS_2022_2')\n\nprint(f\"\\nCategorical features to encode ({len(categorical_features)}):\")\nfor col in categorical_features:\n    if col in df_reg.columns:\n        le = LabelEncoder()\n        df_reg[f'{col}_encoded'] = le.fit_transform(\n            df_reg[col].fillna('Unknown').astype(str)\n        )\n        le_reg[col] = le\n        print(f\"  ‚úì {col}: {df_reg[col].nunique()} unique values\")\n\n# Select features for regression\nfeature_cols = []\nfor col in categorical_features:\n    encoded_col = f'{col}_encoded'\n    if encoded_col in df_reg.columns:\n        feature_cols.append(encoded_col)\n\n# Add numerical features if available\nnumeric_features = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']\nfor num_feat in numeric_features:\n    if num_feat in df_reg.columns:\n        df_reg[num_feat] = pd.to_numeric(df_reg[num_feat], errors='coerce')\n        if df_reg[num_feat].notna().sum() > 0:\n            feature_cols.append(num_feat)\n            print(f\"  ‚úì {num_feat}: numeric feature added\")\n\nprint(f\"\\n Total Features for Salary Prediction: {len(feature_cols)}\")\n\n# Prepare X and y\nX = df_reg[feature_cols].fillna(df_reg[feature_cols].mean())\ny = df_reg['SALARY']\n\nprint(f\"\\n Dataset Statistics:\")\nprint(f\"  ‚Ä¢ Feature Matrix Shape: {X.shape}\")\nprint(f\"  ‚Ä¢ Salary Statistics:\")\nprint(f\"    - Mean: ${y.mean():,.2f}\")\nprint(f\"    - Median: ${y.median():,.2f}\")\nprint(f\"    - Std Dev: ${y.std():,.2f}\")\nprint(f\"    - Range: ${y.min():,.2f} - ${y.max():,.2f}\")\n```\n:::\n\n\nFeature Selection Justification:\nThe features were selected based on their theoretical and empirical relationship with salary:\n\nSTATE & Political Leaning: Geographic location and political climate influence cost of living and compensation policies\nTITLE: Job title is the primary indicator of role level and responsibility\nLIGHTCAST_SECTORS: Industry sector determines baseline compensation structure\nSOC/NAICS: Occupation classification provides standardized job categorization\nYears of Experience: Direct correlation with salary progression (if available)\n\n::: {#train-test-split-regression .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\n# Split data (70/30 as per assignment requirements)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42\n)\n\nprint(\"=\"*80)\nprint(\"TRAIN/TEST SPLIT\")\nprint(\"=\"*80)\nprint(f\"\\nTraining Set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Test Set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"\\nSalary Distribution:\")\nprint(f\"  Training - Mean: ${y_train.mean():,.2f}, Std: ${y_train.std():,.2f}\")\nprint(f\"  Test - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}\")\n```\n:::\n\n\n::: {#multiple-linear-regression .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"View Linear Regression code\"}\nprint(\"=\"*80)\nprint(\"MODEL 1: MULTIPLE LINEAR REGRESSION\")\nprint(\"=\"*80)\n\n# Train model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Make predictions\ny_pred_lin = lin_reg.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))\nr2_lin = r2_score(y_test, y_pred_lin)\nmae_lin = np.mean(np.abs(y_test - y_pred_lin))\n\nprint(f\"\\nüìä Model Performance:\")\nprint(f\"   ‚Ä¢ R¬≤ Score: {r2_lin:.4f}\")\nprint(f\"     ‚Üí Explains {r2_lin*100:.2f}% of salary variance\")\nprint(f\"   ‚Ä¢ RMSE: ${rmse_lin:,.2f}\")\nprint(f\"   ‚Ä¢ MAE: ${mae_lin:,.2f}\")\n\n# Feature coefficients\ncoef_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Coefficient': lin_reg.coef_\n}).sort_values('Coefficient', key=abs, ascending=False)\n\nprint(f\"\\nüìà Top 10 Most Influential Features:\")\nprint(coef_df.head(10).to_string(index=False))\n```\n:::\n\n\n::: {#visualize-linear-coefficients .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\n# Visualize top 15 feature coefficients\ntop_features = coef_df.head(15)\n\nfig = px.bar(\n    top_features,\n    x='Coefficient',\n    y='Feature',\n    orientation='h',\n    title='Top 15 Feature Coefficients - Multiple Linear Regression',\n    labels={'Coefficient': 'Impact on Salary ($)', 'Feature': 'Feature'},\n    color='Coefficient',\n    color_continuous_scale='RdBu',\n    color_continuous_midpoint=0\n)\n\nfig.update_layout(template=\"plotly_white\", height=500, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\nfig.write_html(\"./output/linear_coefficients.html\")\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/linear_coefficients.html\" title=\"Linear Coefficients\"></iframe>\n```\n\n::: {#random-forest-regression .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"View Random Forest code\"}\nprint(\"=\"*80)\nprint(\"MODEL 2: RANDOM FOREST REGRESSION\")\nprint(\"=\"*80)\n\n# Train model\nrf_reg = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=20,\n    min_samples_split=10,\n    min_samples_leaf=4,\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"Training Random Forest model...\")\nrf_reg.fit(X_train, y_train)\nprint(\"‚úì Training complete!\")\n\n# Make predictions\ny_pred_rf = rf_reg.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nr2_rf = r2_score(y_test, y_pred_rf)\nmae_rf = np.mean(np.abs(y_test - y_pred_rf))\n\nprint(f\"\\n Model Performance:\")\nprint(f\"   ‚Ä¢ R¬≤ Score: {r2_rf:.4f}\")\nprint(f\"     ‚Üí Explains {r2_rf*100:.2f}% of salary variance\")\nprint(f\"   ‚Ä¢ RMSE: ${rmse_rf:,.2f}\")\nprint(f\"   ‚Ä¢ MAE: ${mae_rf:,.2f}\")\n\n# Feature importance\nimportance_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Importance': rf_reg.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(f\"\\n Top 10 Most Important Features:\")\nprint(importance_df.head(10).to_string(index=False))\n\n# Calculate improvement safely\nif r2_lin > 0:\n    improvement = ((r2_rf - r2_lin) / r2_lin) * 100\n    print(f\"\\nüöÄ Random Forest improves R¬≤ by {improvement:.1f}% over Linear Regression\")\n```\n:::\n\n\n::: {#visualize-feature-importance .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\n# Visualize feature importance\ntop_features_rf = importance_df.head(15)\n\nfig = px.bar(\n    top_features_rf,\n    x='Importance',\n    y='Feature',\n    orientation='h',\n    title='Top 15 Feature Importance - Random Forest Regression',\n    labels={'Importance': 'Importance Score', 'Feature': 'Feature'},\n    color='Importance',\n    color_continuous_scale='Viridis'\n)\n\nfig.update_layout(template=\"plotly_white\", height=500, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\nfig.write_html(\"./output/feature_importance.html\")\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/feature_importance.html\" title=\"Feature Importance for Random Forest Regression\"></iframe>\n```\n\n::: {#model-comparison-regression .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nprint(\"=\"*80)\nprint(\"REGRESSION MODEL COMPARISON\")\nprint(\"=\"*80)\n\n# Create comparison dataframe\ncomparison_df = pd.DataFrame({\n    'Model': ['Multiple Linear Regression', 'Random Forest Regression'],\n    'R¬≤ Score': [r2_lin, r2_rf],\n    'RMSE ($)': [rmse_lin, rmse_rf],\n    'MAE ($)': [mae_lin, mae_rf]\n})\n\nprint(\"\\n\" + comparison_df.to_string(index=False))\n\nbest_model = comparison_df.loc[comparison_df['R¬≤ Score'].idxmax(), 'Model']\nbest_r2 = comparison_df['R¬≤ Score'].max()\n\nprint(f\"\\nüèÜ Best Performing Model: {best_model}\")\nprint(f\"   ‚Ä¢ Achieves R¬≤ of {best_r2:.4f}\")\nprint(f\"   ‚Ä¢ Explains {best_r2*100:.2f}% of salary variance\")\n```\n:::\n\n\n::: {#predictions-vs-actual .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\n# Sample for performance\nsample_size = min(2000, len(y_test))\nindices = np.random.choice(len(y_test), sample_size, replace=False)\n\ncomparison_results = pd.DataFrame({\n    'Actual': y_test.iloc[indices],\n    'Linear_Regression': y_pred_lin[indices],\n    'Random_Forest': y_pred_rf[indices]\n})\n\nfig = go.Figure()\n\n# Random Forest predictions\nfig.add_trace(go.Scatter(\n    x=comparison_results['Actual'],\n    y=comparison_results['Random_Forest'],\n    mode='markers',\n    name='Random Forest',\n    opacity=0.6,\n    marker=dict(size=5, color='blue')\n))\n\n# Linear Regression predictions\nfig.add_trace(go.Scatter(\n    x=comparison_results['Actual'],\n    y=comparison_results['Linear_Regression'],\n    mode='markers',\n    name='Linear Regression',\n    opacity=0.6,\n    marker=dict(size=5, color='red')\n))\n\n# Perfect prediction line\nmin_val = comparison_results['Actual'].min()\nmax_val = comparison_results['Actual'].max()\n\nfig.add_trace(go.Scatter(\n    x=[min_val, max_val],\n    y=[min_val, max_val],\n    mode='lines',\n    name='Perfect Prediction',\n    line=dict(color='green', dash='dash', width=2)\n))\n\nfig.update_layout(\n    title=f'Actual vs Predicted Salary - Model Comparison (n={sample_size:,})',\n    xaxis_title='Actual Salary ($)',\n    yaxis_title='Predicted Salary ($)',\n    template=\"plotly_white\",\n    height=550,\n    hovermode='closest'\n)\n\nfig.show()\n\nfig.write_html(\"./output/model_comparison.html\")\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/model_comparison.html\" title=\"Model Comparison\"></iframe>\n```\n\n",
    "supporting": [
      "ml_methods_files"
    ],
    "filters": [],
    "includes": {}
  }
}