{
  "hash": "0739b23b618355e5e4d6e79a34eff7ca",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Cleaning\"\nformat:\n  html:\n    code-overflow: wrap\n    code-fold: true\nexecute:\n  echo: true\n  eval: false\n  freeze: auto\n---\n\n## Load the dataset\n\nThis code initializes a PySpark environment to load and explore a dataset of job postings. It begins by importing and starting a Spark session named \"`JobPostingsAnalysis`\", then reads a CSV file (`lightcast_job_postings.csv`) into a Spark DataFrame with headers, schema inference, and support for multi-line fields. The DataFrame is registered as a temporary SQL view called \"`job_postings`\" to enable SQL-style queries. Finally, it performs a basic diagnostic check by printing the schema and previewing the first five rows of dataâ€”steps that are intended for local debugging and should be commented out when rendering the final submission.\n\n::: {#f14c8c31 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom pyspark.sql import SparkSession\nimport re\nimport numpy as np\nimport plotly.graph_objects as go\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\njobs_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\njobs_df.createOrReplaceTempView(\"job_postings\")\n\nelections_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/2024_election_results.csv\")\nelections_df.createOrReplaceTempView(\"election_results\")\n\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\n#jobs_df.show(5)\n#elections_df.show(5)\n```\n:::\n\n\n# Data Cleaning\n\n::: {#1711f4a9 .cell execution_count=2}\n``` {.python .cell-code}\n# casting corrected variable type\njobs_df = jobs_df.withColumn(\"SALARY_FROM\", col (\"SALARY_FROM\").cast(\"float\"))\\\n  .withColumn(\"SALARY_TO\", col(\"SALARY_TO\").cast(\"float\")) \\\n  .withColumn(\"MAX_YEARS_EXPERIENCE\", col(\"MAX_YEARS_EXPERIENCE\").cast(\"float\"))\\\n  .withColumn(\"MIN_YEARS_EXPERIENCE\", col(\"MIN_YEARS_EXPERIENCE\").cast(\"float\"))\\\n  .withColumn(\"SALARY\", col(\"SALARY\").cast(\"float\"))\n\n# Clean Up Columns\njobs_df = jobs_df.withColumn(\"EDUCATION_LEVELS_NAME\", regexp_replace(col(\"EDUCATION_LEVELS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOURCE_TYPES\", regexp_replace(col(\"SOURCE_TYPES\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOURCES\", regexp_replace(col(\"SOURCES\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SKILLS\", regexp_replace(col(\"SKILLS\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SKILLS_NAME\", regexp_replace(col(\"SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SPECIALIZED_SKILLS_NAME\", regexp_replace(col(\"SPECIALIZED_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CERTIFICATIONS_NAME\", regexp_replace(col(\"CERTIFICATIONS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"COMMON_SKILLS_NAME\", regexp_replace(col(\"COMMON_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOFTWARE_SKILLS_NAME\", regexp_replace(col(\"SOFTWARE_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP6_NAME\", regexp_replace(col(\"CIP6_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP4_NAME\", regexp_replace(col(\"CIP4_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP2_NAME\", regexp_replace(col(\"CIP2_NAME\"), \"[\\n\\r]\", \"\"))\n\n\n# Compute and impute Median Salary\ndef compute_median(sdf, col_name):\n  q = sdf.approxQuantile(col_name, [0.5], 0.01)\n  return q[0] if q else None\n\n\nmedian_from = compute_median(jobs_df, \"SALARY_FROM\")\nmedian_to = compute_median(jobs_df, \"SALARY_TO\")\nmedian_salary = compute_median(jobs_df, \"SALARY\")\n\nprint(\"Medians:\", median_from, median_to, median_salary)\n\njobs_df = jobs_df.fillna({\n  \"SALARY_FROM\": median_from,\n  \"SALARY_TO\": median_to,\n  \"SALARY\": median_salary\n})\n\nfrom pyspark.sql.functions import col\njobs_df = jobs_df.withColumn(\n    \"MIDPOINT_SALARY\",\n    (col(\"SALARY_TO\") + col(\"SALARY_FROM\")) / 2\n)\n\n# Dropping unnecessary columns\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\"STATE\",\"COUNTY_OUTGOING\",\"COUNTY_INCOMMING\",\"MSA_OUTGOING\",\"MSA_INCOMING\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\", \"ONET\",\"ONET_2019\",\"CIP6\",\"CIP4\",\"CIP2\",\"SOC_2021_2\",\"SOC_2021_3\",\"SOC_2021_4\",\"SOC_2021_5\",\"SOC_2\", \"SOC_3\", \"SOC_4\",\"SOC_5\", \"NAICS_2022_2\",\"NAICS_2022_3\",\"NAICS_2022_4\",\"NAICS_2022_5\",\"NAICS_2022_6\",\"CITY\",\"COUNTY\",\"MSA\",\"COUNTY_INCOMING\"\n]\njobs_df = jobs_df.drop(*columns_to_drop)\n\n# configuring remote work groups\nfrom pyspark.sql.functions import when, col, trim\n\njobs_df = jobs_df.withColumn(\"REMOTE_GROUP\",\n  when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Remote\", \"Remote\")\n  .when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Hybrid Remote\", \"Hybrid\")\n  .when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Not Remote\", \"Onsite\")\n  .when(col(\"REMOTE_TYPE_NAME\").isNull(), \"Onsite\")\n  .otherwise(\"Onsite\")\n)\n\n# dropping any duplicate postings\njobs_df = jobs_df.dropDuplicates([\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])\n\n# handling missing data\nfrom pyspark.sql.functions import col, when, sum as spark_sum\n\ntotal_rows = jobs_df.count()\nmissing_threshold = total_rows * 0.5\nnull_counts = jobs_df.select([\n    (spark_sum(col(c).isNull().cast(\"int\"))).alias(c) for c in jobs_df.columns\n]).collect()[0].asDict()\ncolumns_to_keep = [c for c, nulls in null_counts.items() if nulls <= missing_threshold or c == \"SALARY\"]\njobs_df = jobs_df.select(columns_to_keep)\n\n#jobs_df.show(15)\n```\n:::\n\n\n::: {#402872da .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as F\n\njobs_df = jobs_df.withColumn(\"STATE_ABBREVIATION\", F.trim(F.split(jobs_df[\"COUNTY_NAME\"], \",\").getItem(1)))\n\njobs_alias = jobs_df.alias(\"jobs\")\nelections_alias = elections_df.alias(\"elections\")\n\njobs_df = jobs_alias.join(\n    elections_alias,\n    F.col(\"jobs.STATE_ABBREVIATION\") == F.col(\"elections.STATE\"),\n    \"left\"\n)\njobs_df = jobs_df.drop(F.col(\"elections.STATE\"))\n\njobs_df = jobs_df.withColumnRenamed(\"Affiliation\", \"AFFILIATION\")\n\n#jobs_df.show(15)\n```\n:::\n\n\n::: {#93fa5e45 .cell execution_count=4}\n``` {.python .cell-code}\nselected_df = jobs_df.select(\n  \"EDUCATION_LEVELS_NAME\",\n  \"MIN_EDULEVELS_NAME\",\n  \"EMPLOYMENT_TYPE_NAME\",\n  \"MIN_YEARS_EXPERIENCE\",\n  \"SALARY_TO\",\n  \"SALARY_FROM\",\n  \"SALARY\",\n  \"CITY_NAME\",\n  \"MSA_NAME\",\n  \"STATE_NAME\",\n  \"NAICS2_NAME\",\n  \"NAICS3_NAME\",\n  \"NAICS4_NAME\",\n  \"NAICS5_NAME\",\n  \"NAICS6_NAME\",\n  \"SKILLS_NAME\",\n  \"SPECIALIZED_SKILLS_NAME\",\n  \"CERTIFICATIONS_NAME\",\n  \"COMMON_SKILLS_NAME\",\n  \"SOFTWARE_SKILLS_NAME\",\n  \"ONET_NAME\",\n  \"LOT_CAREER_AREA_NAME\",\n  \"LOT_OCCUPATION_NAME\",\n  \"LOT_SPECIALIZED_OCCUPATION_NAME\",\n  \"LOT_OCCUPATION_GROUP_NAME\",\n  \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n  \"LOT_V6_OCCUPATION_NAME\",\n  \"LOT_V6_OCCUPATION_GROUP_NAME\",\n  \"LOT_V6_CAREER_AREA_NAME\",\n  \"SOC_2_NAME\",\n  \"SOC_3_NAME\",\n  \"SOC_4_NAME\",\n  \"SOC_5_NAME\",\n  \"REMOTE_GROUP\",\n  \"STATE_ABBREVIATION\",\n  \"AFFILIATION\",\n  \"MIDPOINT_SALARY\"\n)\n```\n:::\n\n\n::: {#5a241e92 .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nfrom pyspark.sql.functions import col, sum as spark_sum, when, trim, length\nimport hvplot.pandas\n\ndf_sample_viz = selected_df.select(\n  \"MIN_YEARS_EXPERIENCE\",\n  \"SALARY\",\n  \"MSA_NAME\",\n  \"NAICS5_NAME\"\n)\n\ndf_sample = df_sample_viz.sample(fraction = .15, seed = 42).toPandas()\n\nmissing_mask = df_sample.isnull()\n\nmissing_long = (\n  missing_mask.reset_index()\n  .melt(id_vars = \"index\", var_name = \"column\", value_name = \"is_missing\")\n)\n\nmissing_long[\"is_missing\"] = missing_long[\"is_missing\"].astype(int)\n\nmissing = missing_long.hvplot.heatmap(\n  x=\"column\",\n  y=\"index\",\n  C = \"is_missing\",\n  cmap = \"Blues\",\n  width = 900,\n  height = 500,\n  title = \"Heatmap of Missing Values (15%)\"\n).opts(xrotation=45)\n\nhvplot.save(missing, './output/missing_heatmap.html')\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/missing_heatmap.html\" title=\"Missing Values Heatmap\"></iframe>\n```\n\n::: {#5e981ca5 .cell execution_count=6}\n``` {.python .cell-code}\nfrom pyspark.sql.functions import countDistinct\n\nselected_df.select([\n  countDistinct(c).alias(c+\"_nunique\")\n  for c in selected_df.columns\n]).show(truncate=False)\n\n# Education Levels\n\nselected_df = selected_df.withColumn(\n  \"EDUCATION_LEVELS_NAME\",\n    when(col(\"EDUCATION_LEVELS_NAME\").isNull(), \"No Education Listed\")\n    .otherwise(col(\"EDUCATION_LEVELS_NAME\"))\n)\n\n# Min Edu Levels\n\nselected_df = selected_df.withColumn(\n  \"MIN_EDULEVELS_NAME\",\n    when(col(\"MIN_EDULEVELS_NAME\").isNull(), \"No Education Listed\")\n    .otherwise(col(\"MIN_EDULEVELS_NAME\"))\n)\n\n# Employment Type Name\n\nselected_df = selected_df.withColumn(\n  \"EMPLOYMENT_TYPE_NAME\",\n    when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Part-time / full-time\",\"Flexible\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Part-time (Ã¢â€°Â¤ 32 hours)\",\"Part-Time\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Full-time (> 32 hours)\",\"Full-Time\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\").isNull(), \"Full-Time\")\n    .otherwise(col(\"EMPLOYMENT_TYPE_NAME\"))\n)\n\n# Min Years Experience\nselected_df = selected_df.withColumn(\n    \"MIN_YEARS_EXPERIENCE\",\n    when(col(\"MIN_YEARS_EXPERIENCE\").isNull(), 0)\n    .otherwise(col(\"MIN_YEARS_EXPERIENCE\"))\n)\n\n# Salary to\nselected_df = selected_df.withColumn(\n    \"SALARY_TO\",\n    when(col(\"SALARY_TO\").isNull(), median_to)\n    .otherwise(col(\"SALARY_TO\"))\n)\n\n# Salary from\nselected_df = selected_df.withColumn(\n    \"SALARY_FROM\",\n    when(col(\"SALARY_FROM\").isNull(), median_from)\n    .otherwise(col(\"SALARY_FROM\"))\n)\n\n# Salary \nselected_df = selected_df.withColumn(\n    \"SALARY\",\n    when(col(\"SALARY\").isNull(), median_salary)\n    .otherwise(col(\"SALARY\"))\n)\n\n# City Name\nselected_df = selected_df.withColumn(\n  \"CITY_NAME\",\n    when(col(\"CITY_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"CITY_NAME\"))\n)\n\n# MSA Name\nselected_df = selected_df.withColumn(\n  \"MSA_NAME\",\n    when(col(\"MSA_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"MSA_NAME\"))\n)\n\n# State Name\nselected_df = selected_df.withColumn(\n  \"STATE_NAME\",\n    when(col(\"STATE_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"STATE_NAME\"))\n)\n\n# NAICS2_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS2_NAME\",\n    when(col(\"NAICS2_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS2_NAME\"))\n)\n\n# NAICS3_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS3_NAME\",\n    when(col(\"NAICS3_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS3_NAME\"))\n)\n\n# NAICS4_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS4_NAME\",\n    when(col(\"NAICS4_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS4_NAME\"))\n)\n\n# NAICS5_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS5_NAME\",\n    when(col(\"NAICS5_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS5_NAME\"))\n)\n\n# NAICS6_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS6_NAME\",\n    when(col(\"NAICS6_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS6_NAME\"))\n)\n\n#STATE ABBREVIATION\nselected_df = selected_df.withColumn(\n  \"STATE_ABBREVIATION\",\n    when(col(\"STATE_ABBREVIATION\").isNull(), \"Unknown\")\n    .otherwise(col(\"STATE_ABBREVIATION\"))\n)\n```\n:::\n\n\n::: {#3d8d53f1 .cell execution_count=7}\n``` {.python .cell-code}\npdf = selected_df.toPandas()\n\npdf.to_csv(\"./data/lightcast_cleaned.csv\", index=False)\n\npdf.head(15)\n\nprint(\"Data Cleaning Complete. Rows retained:\", len(pdf))\n```\n:::\n\n\n",
    "supporting": [
      "data_cleaning_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}