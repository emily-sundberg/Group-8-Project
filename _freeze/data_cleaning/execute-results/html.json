{
  "hash": "0739b23b618355e5e4d6e79a34eff7ca",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Cleaning\"\nformat:\n  html:\n    code-overflow: wrap\n    code-fold: true\nexecute:\n  echo: true\n  eval: false\n  freeze: auto\n---\n\n## Load the dataset\n\nThis code initializes a PySpark environment to load and explore a dataset of job postings. It begins by importing and starting a Spark session named \"`JobPostingsAnalysis`\", then reads a CSV file (`lightcast_job_postings.csv`) into a Spark DataFrame with headers, schema inference, and support for multi-line fields. The DataFrame is registered as a temporary SQL view called \"`job_postings`\" to enable SQL-style queries. Finally, it performs a basic diagnostic check by printing the schema and previewing the first five rows of data—steps that are intended for local debugging and should be commented out when rendering the final submission.\n\n::: {#f14c8c31 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom pyspark.sql import SparkSession\nimport re\nimport numpy as np\nimport plotly.graph_objects as go\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\njobs_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\njobs_df.createOrReplaceTempView(\"job_postings\")\n\nelections_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/2024_election_results.csv\")\nelections_df.createOrReplaceTempView(\"election_results\")\n\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\n#jobs_df.show(5)\n#elections_df.show(5)\n```\n:::\n\n\n# Data Cleaning\n\n::: {#1711f4a9 .cell execution_count=2}\n``` {.python .cell-code}\n# casting corrected variable type\njobs_df = jobs_df.withColumn(\"SALARY_FROM\", col (\"SALARY_FROM\").cast(\"float\"))\\\n  .withColumn(\"SALARY_TO\", col(\"SALARY_TO\").cast(\"float\")) \\\n  .withColumn(\"MAX_YEARS_EXPERIENCE\", col(\"MAX_YEARS_EXPERIENCE\").cast(\"float\"))\\\n  .withColumn(\"MIN_YEARS_EXPERIENCE\", col(\"MIN_YEARS_EXPERIENCE\").cast(\"float\"))\\\n  .withColumn(\"SALARY\", col(\"SALARY\").cast(\"float\"))\n\n# Clean Up Columns\njobs_df = jobs_df.withColumn(\"EDUCATION_LEVELS_NAME\", regexp_replace(col(\"EDUCATION_LEVELS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOURCE_TYPES\", regexp_replace(col(\"SOURCE_TYPES\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOURCES\", regexp_replace(col(\"SOURCES\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SKILLS\", regexp_replace(col(\"SKILLS\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SKILLS_NAME\", regexp_replace(col(\"SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SPECIALIZED_SKILLS_NAME\", regexp_replace(col(\"SPECIALIZED_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CERTIFICATIONS_NAME\", regexp_replace(col(\"CERTIFICATIONS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"COMMON_SKILLS_NAME\", regexp_replace(col(\"COMMON_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOFTWARE_SKILLS_NAME\", regexp_replace(col(\"SOFTWARE_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP6_NAME\", regexp_replace(col(\"CIP6_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP4_NAME\", regexp_replace(col(\"CIP4_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP2_NAME\", regexp_replace(col(\"CIP2_NAME\"), \"[\\n\\r]\", \"\"))\n\n\n# Compute and impute Median Salary\ndef compute_median(sdf, col_name):\n  q = sdf.approxQuantile(col_name, [0.5], 0.01)\n  return q[0] if q else None\n\n\nmedian_from = compute_median(jobs_df, \"SALARY_FROM\")\nmedian_to = compute_median(jobs_df, \"SALARY_TO\")\nmedian_salary = compute_median(jobs_df, \"SALARY\")\n\nprint(\"Medians:\", median_from, median_to, median_salary)\n\njobs_df = jobs_df.fillna({\n  \"SALARY_FROM\": median_from,\n  \"SALARY_TO\": median_to,\n  \"SALARY\": median_salary\n})\n\nfrom pyspark.sql.functions import col\njobs_df = jobs_df.withColumn(\n    \"MIDPOINT_SALARY\",\n    (col(\"SALARY_TO\") + col(\"SALARY_FROM\")) / 2\n)\n\n# Dropping unnecessary columns\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\"STATE\",\"COUNTY_OUTGOING\",\"COUNTY_INCOMMING\",\"MSA_OUTGOING\",\"MSA_INCOMING\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\", \"ONET\",\"ONET_2019\",\"CIP6\",\"CIP4\",\"CIP2\",\"SOC_2021_2\",\"SOC_2021_3\",\"SOC_2021_4\",\"SOC_2021_5\",\"SOC_2\", \"SOC_3\", \"SOC_4\",\"SOC_5\", \"NAICS_2022_2\",\"NAICS_2022_3\",\"NAICS_2022_4\",\"NAICS_2022_5\",\"NAICS_2022_6\",\"CITY\",\"COUNTY\",\"MSA\",\"COUNTY_INCOMING\"\n]\njobs_df = jobs_df.drop(*columns_to_drop)\n\n# configuring remote work groups\nfrom pyspark.sql.functions import when, col, trim\n\njobs_df = jobs_df.withColumn(\"REMOTE_GROUP\",\n  when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Remote\", \"Remote\")\n  .when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Hybrid Remote\", \"Hybrid\")\n  .when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Not Remote\", \"Onsite\")\n  .when(col(\"REMOTE_TYPE_NAME\").isNull(), \"Onsite\")\n  .otherwise(\"Onsite\")\n)\n\n# dropping any duplicate postings\njobs_df = jobs_df.dropDuplicates([\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])\n\n# handling missing data\nfrom pyspark.sql.functions import col, when, sum as spark_sum\n\ntotal_rows = jobs_df.count()\nmissing_threshold = total_rows * 0.5\nnull_counts = jobs_df.select([\n    (spark_sum(col(c).isNull().cast(\"int\"))).alias(c) for c in jobs_df.columns\n]).collect()[0].asDict()\ncolumns_to_keep = [c for c, nulls in null_counts.items() if nulls <= missing_threshold or c == \"SALARY\"]\njobs_df = jobs_df.select(columns_to_keep)\n\n#jobs_df.show(15)\n```\n:::\n\n\n::: {#402872da .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as F\n\njobs_df = jobs_df.withColumn(\"STATE_ABBREVIATION\", F.trim(F.split(jobs_df[\"COUNTY_NAME\"], \",\").getItem(1)))\n\njobs_alias = jobs_df.alias(\"jobs\")\nelections_alias = elections_df.alias(\"elections\")\n\njobs_df = jobs_alias.join(\n    elections_alias,\n    F.col(\"jobs.STATE_ABBREVIATION\") == F.col(\"elections.STATE\"),\n    \"left\"\n)\njobs_df = jobs_df.drop(F.col(\"elections.STATE\"))\n\njobs_df = jobs_df.withColumnRenamed(\"Affiliation\", \"AFFILIATION\")\n\n#jobs_df.show(15)\n```\n:::\n\n\n::: {#93fa5e45 .cell execution_count=4}\n``` {.python .cell-code}\nselected_df = jobs_df.select(\n  \"EDUCATION_LEVELS_NAME\",\n  \"MIN_EDULEVELS_NAME\",\n  \"EMPLOYMENT_TYPE_NAME\",\n  \"MIN_YEARS_EXPERIENCE\",\n  \"SALARY_TO\",\n  \"SALARY_FROM\",\n  \"SALARY\",\n  \"CITY_NAME\",\n  \"MSA_NAME\",\n  \"STATE_NAME\",\n  \"NAICS2_NAME\",\n  \"NAICS3_NAME\",\n  \"NAICS4_NAME\",\n  \"NAICS5_NAME\",\n  \"NAICS6_NAME\",\n  \"SKILLS_NAME\",\n  \"SPECIALIZED_SKILLS_NAME\",\n  \"CERTIFICATIONS_NAME\",\n  \"COMMON_SKILLS_NAME\",\n  \"SOFTWARE_SKILLS_NAME\",\n  \"ONET_NAME\",\n  \"LOT_CAREER_AREA_NAME\",\n  \"LOT_OCCUPATION_NAME\",\n  \"LOT_SPECIALIZED_OCCUPATION_NAME\",\n  \"LOT_OCCUPATION_GROUP_NAME\",\n  \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n  \"LOT_V6_OCCUPATION_NAME\",\n  \"LOT_V6_OCCUPATION_GROUP_NAME\",\n  \"LOT_V6_CAREER_AREA_NAME\",\n  \"SOC_2_NAME\",\n  \"SOC_3_NAME\",\n  \"SOC_4_NAME\",\n  \"SOC_5_NAME\",\n  \"REMOTE_GROUP\",\n  \"STATE_ABBREVIATION\",\n  \"AFFILIATION\",\n  \"MIDPOINT_SALARY\"\n)\n```\n:::\n\n\n::: {#5a241e92 .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nfrom pyspark.sql.functions import col, sum as spark_sum, when, trim, length\nimport hvplot.pandas\n\ndf_sample_viz = selected_df.select(\n  \"MIN_YEARS_EXPERIENCE\",\n  \"SALARY\",\n  \"MSA_NAME\",\n  \"NAICS5_NAME\"\n)\n\ndf_sample = df_sample_viz.sample(fraction = .15, seed = 42).toPandas()\n\nmissing_mask = df_sample.isnull()\n\nmissing_long = (\n  missing_mask.reset_index()\n  .melt(id_vars = \"index\", var_name = \"column\", value_name = \"is_missing\")\n)\n\nmissing_long[\"is_missing\"] = missing_long[\"is_missing\"].astype(int)\n\nmissing = missing_long.hvplot.heatmap(\n  x=\"column\",\n  y=\"index\",\n  C = \"is_missing\",\n  cmap = \"Blues\",\n  width = 900,\n  height = 500,\n  title = \"Heatmap of Missing Values (15%)\"\n).opts(xrotation=45)\n\nhvplot.save(missing, './output/missing_heatmap.html')\n```\n:::\n\n\n```{=html}\n<iframe width=\"900\" height=\"650\" src=\"./output/missing_heatmap.html\" title=\"Missing Values Heatmap\"></iframe>\n```\n\n::: {#5e981ca5 .cell execution_count=6}\n``` {.python .cell-code}\nfrom pyspark.sql.functions import countDistinct\n\nselected_df.select([\n  countDistinct(c).alias(c+\"_nunique\")\n  for c in selected_df.columns\n]).show(truncate=False)\n\n# Education Levels\n\nselected_df = selected_df.withColumn(\n  \"EDUCATION_LEVELS_NAME\",\n    when(col(\"EDUCATION_LEVELS_NAME\").isNull(), \"No Education Listed\")\n    .otherwise(col(\"EDUCATION_LEVELS_NAME\"))\n)\n\n# Min Edu Levels\n\nselected_df = selected_df.withColumn(\n  \"MIN_EDULEVELS_NAME\",\n    when(col(\"MIN_EDULEVELS_NAME\").isNull(), \"No Education Listed\")\n    .otherwise(col(\"MIN_EDULEVELS_NAME\"))\n)\n\n# Employment Type Name\n\nselected_df = selected_df.withColumn(\n  \"EMPLOYMENT_TYPE_NAME\",\n    when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Part-time / full-time\",\"Flexible\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Part-time (â‰¤ 32 hours)\",\"Part-Time\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Full-time (> 32 hours)\",\"Full-Time\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\").isNull(), \"Full-Time\")\n    .otherwise(col(\"EMPLOYMENT_TYPE_NAME\"))\n)\n\n# Min Years Experience\nselected_df = selected_df.withColumn(\n    \"MIN_YEARS_EXPERIENCE\",\n    when(col(\"MIN_YEARS_EXPERIENCE\").isNull(), 0)\n    .otherwise(col(\"MIN_YEARS_EXPERIENCE\"))\n)\n\n# Salary to\nselected_df = selected_df.withColumn(\n    \"SALARY_TO\",\n    when(col(\"SALARY_TO\").isNull(), median_to)\n    .otherwise(col(\"SALARY_TO\"))\n)\n\n# Salary from\nselected_df = selected_df.withColumn(\n    \"SALARY_FROM\",\n    when(col(\"SALARY_FROM\").isNull(), median_from)\n    .otherwise(col(\"SALARY_FROM\"))\n)\n\n# Salary \nselected_df = selected_df.withColumn(\n    \"SALARY\",\n    when(col(\"SALARY\").isNull(), median_salary)\n    .otherwise(col(\"SALARY\"))\n)\n\n# City Name\nselected_df = selected_df.withColumn(\n  \"CITY_NAME\",\n    when(col(\"CITY_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"CITY_NAME\"))\n)\n\n# MSA Name\nselected_df = selected_df.withColumn(\n  \"MSA_NAME\",\n    when(col(\"MSA_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"MSA_NAME\"))\n)\n\n# State Name\nselected_df = selected_df.withColumn(\n  \"STATE_NAME\",\n    when(col(\"STATE_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"STATE_NAME\"))\n)\n\n# NAICS2_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS2_NAME\",\n    when(col(\"NAICS2_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS2_NAME\"))\n)\n\n# NAICS3_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS3_NAME\",\n    when(col(\"NAICS3_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS3_NAME\"))\n)\n\n# NAICS4_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS4_NAME\",\n    when(col(\"NAICS4_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS4_NAME\"))\n)\n\n# NAICS5_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS5_NAME\",\n    when(col(\"NAICS5_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS5_NAME\"))\n)\n\n# NAICS6_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS6_NAME\",\n    when(col(\"NAICS6_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS6_NAME\"))\n)\n\n#STATE ABBREVIATION\nselected_df = selected_df.withColumn(\n  \"STATE_ABBREVIATION\",\n    when(col(\"STATE_ABBREVIATION\").isNull(), \"Unknown\")\n    .otherwise(col(\"STATE_ABBREVIATION\"))\n)\n```\n:::\n\n\n::: {#3d8d53f1 .cell execution_count=7}\n``` {.python .cell-code}\npdf = selected_df.toPandas()\n\npdf.to_csv(\"./data/lightcast_cleaned.csv\", index=False)\n\npdf.head(15)\n\nprint(\"Data Cleaning Complete. Rows retained:\", len(pdf))\n```\n:::\n\n\n",
    "supporting": [
      "data_cleaning_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}