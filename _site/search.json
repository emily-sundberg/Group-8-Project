[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Project",
    "section": "",
    "text": "This project examines the complex relationship between gender disparities in employment, hiring patterns, wage gaps, and political geography across the United States. We pay particular attention to representation in AI and technology fields."
  },
  {
    "objectID": "about.html#research-focus",
    "href": "about.html#research-focus",
    "title": "About This Project",
    "section": "",
    "text": "This project examines the complex relationship between gender disparities in employment, hiring patterns, wage gaps, and political geography across the United States. We pay particular attention to representation in AI and technology fields."
  },
  {
    "objectID": "about.html#our-four-core-questions",
    "href": "about.html#our-four-core-questions",
    "title": "About This Project",
    "section": "Our Four Core Questions",
    "text": "Our Four Core Questions\n\nHiring Patterns: How do men and women fare differently in hiring across various industries?\nPolitical Geography: Do employment disparities correlate with state political affiliations (red vs. blue states)?\nAI Sector Focus: Is women’s underrepresentation in AI fields more pronounced in conservative states?\nWage Analysis: How do gender wage gaps vary across different political contexts?"
  },
  {
    "objectID": "about.html#why-this-matters",
    "href": "about.html#why-this-matters",
    "title": "About This Project",
    "section": "Why This Matters",
    "text": "Why This Matters\n\nAI and tech sectors are rapidly growing and high-paying\nPolitical policies at state level impact workplace equality\nUnderstanding regional differences can inform policy decisions\nIdentifying barriers helps create targeted interventions"
  },
  {
    "objectID": "about.html#methodology",
    "href": "about.html#methodology",
    "title": "About This Project",
    "section": "Methodology",
    "text": "Methodology\nOur analysis will include:\n\nData Sources:\n\nBureau of Labor Statistics employment data\nState-level political affiliation data\nIndustry-specific hiring and salary information\nAI/tech sector demographics\n\nAnalytical Approach:\n\nStatistical analysis of hiring patterns\nGeographic mapping of disparities\nCorrelation analysis between political affiliation and employment outcomes\nComparative analysis across industries"
  },
  {
    "objectID": "about.html#team",
    "href": "about.html#team",
    "title": "About This Project",
    "section": "Team",
    "text": "Team\nGroup 8 - AD 688 Web Analytics\n\nPranathi Nagasai\nEmily Sundberg"
  },
  {
    "objectID": "about.html#course-information",
    "href": "about.html#course-information",
    "title": "About This Project",
    "section": "Course Information",
    "text": "Course Information\nBoston University Metropolitan College\nAD 688: Web Analytics\nFall 2025"
  },
  {
    "objectID": "about.html#timeline",
    "href": "about.html#timeline",
    "title": "About This Project",
    "section": "Timeline",
    "text": "Timeline\n\nData collection and cleaning\nExploratory data analysis\nStatistical modeling\nVisualization development\nFinal report and presentation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gender Disparities in Hiring & Political Influence",
    "section": "",
    "text": "This research investigates the intersection of gender-based employment patterns and political geography in the United States.\n\n\n\n\nWe analyze gender representation and hiring trends across multiple sectors:\n\nTechnology Sector: Software development, AI/ML, data science\nHealthcare: Medical professionals, administrative roles\nFinance: Banking, investment, fintech\nManufacturing: Production, engineering, management\nService Industries: Retail, hospitality, education\n\nKey metrics examined: - Hiring rates by gender - Retention rates - Promotion patterns - Industry-specific barriers\n\n\n\nExamining state-level political affiliations and their relationship with:\n\nRed States (Conservative-leaning)\n\nGender hiring ratios\nWorkplace policies\nWage structures\n\nBlue States (Liberal-leaning)\n\nGender hiring ratios\nWorkplace policies\nWage structures\n\nSwing States: Comparative analysis of mixed political environments\n\n\n\n\nFocused investigation on the technology sector, specifically:\n\nAI and Machine Learning roles\n\nData scientists\nML engineers\nAI researchers\n\nWomen’s representation comparison:\n\nConservative states (red states)\nLiberal states (blue states)\nNational averages\n\nFactors analyzed:\n\nEducational pipeline differences\nCompany culture and policies\nState-level STEM initiatives\nIndustry concentration by state\n\n\n\n\n\nComprehensive wage analysis examining:\n\nGender wage gaps across:\n\nRed states\nBlue states\nSwing states\n\nIndustry-specific wage disparities:\n\nTech and AI fields\nHealthcare\nFinance\nManufacturing\n\nControlling for:\n\nExperience level\nEducation\nJob title/role\nCompany size\nCost of living adjustments\n\nPolitical affiliation impact:\n\nState minimum wage policies\nEqual pay legislation\nWorkplace protection laws\n\n\n\n\n\n\nWe expect this analysis will reveal that political geography influences gender-based employment outcomes, particularly in emerging high-wage sectors such as AI and technology.\n\n\n\n[Your data sources and analytical methods will go here]\n\n\n\n[Interactive charts and graphs will be added here]\n\n\n\n[Research findings will be presented here]"
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Gender Disparities in Hiring & Political Influence",
    "section": "",
    "text": "We analyze gender representation and hiring trends across multiple sectors:\n\nTechnology Sector: Software development, AI/ML, data science\nHealthcare: Medical professionals, administrative roles\nFinance: Banking, investment, fintech\nManufacturing: Production, engineering, management\nService Industries: Retail, hospitality, education\n\nKey metrics examined: - Hiring rates by gender - Retention rates - Promotion patterns - Industry-specific barriers\n\n\n\nExamining state-level political affiliations and their relationship with:\n\nRed States (Conservative-leaning)\n\nGender hiring ratios\nWorkplace policies\nWage structures\n\nBlue States (Liberal-leaning)\n\nGender hiring ratios\nWorkplace policies\nWage structures\n\nSwing States: Comparative analysis of mixed political environments\n\n\n\n\nFocused investigation on the technology sector, specifically:\n\nAI and Machine Learning roles\n\nData scientists\nML engineers\nAI researchers\n\nWomen’s representation comparison:\n\nConservative states (red states)\nLiberal states (blue states)\nNational averages\n\nFactors analyzed:\n\nEducational pipeline differences\nCompany culture and policies\nState-level STEM initiatives\nIndustry concentration by state\n\n\n\n\n\nComprehensive wage analysis examining:\n\nGender wage gaps across:\n\nRed states\nBlue states\nSwing states\n\nIndustry-specific wage disparities:\n\nTech and AI fields\nHealthcare\nFinance\nManufacturing\n\nControlling for:\n\nExperience level\nEducation\nJob title/role\nCompany size\nCost of living adjustments\n\nPolitical affiliation impact:\n\nState minimum wage policies\nEqual pay legislation\nWorkplace protection laws"
  },
  {
    "objectID": "index.html#expected-findings",
    "href": "index.html#expected-findings",
    "title": "Gender Disparities in Hiring & Political Influence",
    "section": "",
    "text": "We expect this analysis will reveal that political geography influences gender-based employment outcomes, particularly in emerging high-wage sectors such as AI and technology."
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Gender Disparities in Hiring & Political Influence",
    "section": "",
    "text": "[Your data sources and analytical methods will go here]"
  },
  {
    "objectID": "index.html#data-visualizations",
    "href": "index.html#data-visualizations",
    "title": "Gender Disparities in Hiring & Political Influence",
    "section": "",
    "text": "[Interactive charts and graphs will be added here]"
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "Gender Disparities in Hiring & Political Influence",
    "section": "",
    "text": "[Research findings will be presented here]"
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Load the dataset",
    "section": "",
    "text": "import pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom pyspark.sql import SparkSession\nimport re\nimport numpy as np\nimport plotly.graph_objects as go\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"/home/ubuntu/Group-8-Project/lightcast_job_postings.csv\")\ndf.createOrReplaceTempView(\"job_postings\")\n\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\ndf.show(5)\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/06 17:21:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/10/06 17:21:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n[Stage 1:&gt;                                                          (0 + 1) / 1]                                                                                25/10/06 17:21:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 2:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|                  ID|LAST_UPDATED_DATE|LAST_UPDATED_TIMESTAMP|DUPLICATES|  POSTED|  EXPIRED|DURATION|        SOURCE_TYPES|             SOURCES|                 URL|ACTIVE_URLS|ACTIVE_SOURCES_INFO|           TITLE_RAW|                BODY|MODELED_EXPIRED|MODELED_DURATION| COMPANY|        COMPANY_NAME|COMPANY_RAW|COMPANY_IS_STAFFING|EDUCATION_LEVELS|EDUCATION_LEVELS_NAME|MIN_EDULEVELS| MIN_EDULEVELS_NAME|MAX_EDULEVELS|MAX_EDULEVELS_NAME|EMPLOYMENT_TYPE|EMPLOYMENT_TYPE_NAME|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|IS_INTERNSHIP|SALARY|REMOTE_TYPE|REMOTE_TYPE_NAME|ORIGINAL_PAY_PERIOD|SALARY_TO|SALARY_FROM|            LOCATION|                CITY|    CITY_NAME|COUNTY|   COUNTY_NAME|  MSA|            MSA_NAME|STATE|STATE_NAME|COUNTY_OUTGOING|COUNTY_NAME_OUTGOING|COUNTY_INCOMING|COUNTY_NAME_INCOMING|MSA_OUTGOING|   MSA_NAME_OUTGOING|MSA_INCOMING|   MSA_NAME_INCOMING|NAICS2|         NAICS2_NAME|NAICS3|         NAICS3_NAME|NAICS4|         NAICS4_NAME|NAICS5|         NAICS5_NAME|NAICS6|         NAICS6_NAME|             TITLE|         TITLE_NAME|         TITLE_CLEAN|              SKILLS|         SKILLS_NAME|  SPECIALIZED_SKILLS|SPECIALIZED_SKILLS_NAME|      CERTIFICATIONS| CERTIFICATIONS_NAME|       COMMON_SKILLS|  COMMON_SKILLS_NAME|     SOFTWARE_SKILLS|SOFTWARE_SKILLS_NAME|      ONET|           ONET_NAME| ONET_2019|      ONET_2019_NAME|                CIP6|           CIP6_NAME|                CIP4|           CIP4_NAME|                CIP2|           CIP2_NAME|SOC_2021_2|     SOC_2021_2_NAME|SOC_2021_3|     SOC_2021_3_NAME|SOC_2021_4|SOC_2021_4_NAME|SOC_2021_5|SOC_2021_5_NAME|LOT_CAREER_AREA|LOT_CAREER_AREA_NAME|LOT_OCCUPATION| LOT_OCCUPATION_NAME|LOT_SPECIALIZED_OCCUPATION|LOT_SPECIALIZED_OCCUPATION_NAME|LOT_OCCUPATION_GROUP|LOT_OCCUPATION_GROUP_NAME|LOT_V6_SPECIALIZED_OCCUPATION|LOT_V6_SPECIALIZED_OCCUPATION_NAME|LOT_V6_OCCUPATION|LOT_V6_OCCUPATION_NAME|LOT_V6_OCCUPATION_GROUP|LOT_V6_OCCUPATION_GROUP_NAME|LOT_V6_CAREER_AREA|LOT_V6_CAREER_AREA_NAME|  SOC_2|          SOC_2_NAME|  SOC_3|          SOC_3_NAME|  SOC_4|     SOC_4_NAME|  SOC_5|     SOC_5_NAME|LIGHTCAST_SECTORS|LIGHTCAST_SECTORS_NAME|NAICS_2022_2|   NAICS_2022_2_NAME|NAICS_2022_3|   NAICS_2022_3_NAME|NAICS_2022_4|   NAICS_2022_4_NAME|NAICS_2022_5|   NAICS_2022_5_NAME|NAICS_2022_6|   NAICS_2022_6_NAME|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|1f57d95acf4dc67ed...|         9/6/2024|  2024-09-06 20:32:...|         0|6/2/2024| 6/8/2024|       6|   [\\n  \"Company\"\\n]|[\\n  \"brassring.c...|[\\n  \"https://sjo...|         []|               NULL|Enterprise Analys...|31-May-2024\\n\\nEn...|       6/8/2024|               6|  894731|          Murphy USA| Murphy USA|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   2|                   2|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.20...|RWwgRG9yYWRvLCBBUg==|El Dorado, AR|  5139|     Union, AR|20980|       El Dorado, AR|    5|  Arkansas|           5139|           Union, AR|           5139|           Union, AR|       20980|       El Dorado, AR|       20980|       El Dorado, AR|    44|        Retail Trade|   441|Motor Vehicle and...|  4413|Automotive Parts,...| 44133|Automotive Parts ...|441330|Automotive Parts ...|ET29C073C03D1F86B4|Enterprise Analysts|enterprise analys...|[\\n  \"KS126DB6T06...|[\\n  \"Merchandisi...|[\\n  \"KS126DB6T06...|   [\\n  \"Merchandisi...|                  []|                  []|[\\n  \"KS126706DPF...|[\\n  \"Mathematics...|[\\n  \"KS440W865GC...|[\\n  \"SQL (Progra...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|[\\n  \"45.0601\",\\n...|[\\n  \"Economics, ...|[\\n  \"45.06\",\\n  ...|[\\n  \"Economics\",...|[\\n  \"45\",\\n  \"27...|[\\n  \"Social Scie...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  7\\n]|  [\\n  \"Artificial ...|          44|        Retail Trade|         441|Motor Vehicle and...|        4413|Automotive Parts,...|       44133|Automotive Parts ...|      441330|Automotive Parts ...|\n|0cb072af26757b6c4...|         8/2/2024|  2024-08-02 17:08:...|         0|6/2/2024| 8/1/2024|    NULL| [\\n  \"Job Board\"\\n]| [\\n  \"maine.gov\"\\n]|[\\n  \"https://job...|         []|               NULL|Oracle Consultant...|Oracle Consultant...|       8/1/2024|            NULL|  133098|Smx Corporation L...|        SMX|               true|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                   3|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|{\\n  \"lat\": 44.31...|    QXVndXN0YSwgTUU=|  Augusta, ME| 23011|  Kennebec, ME|12300|Augusta-Watervill...|   23|     Maine|          23011|        Kennebec, ME|          23011|        Kennebec, ME|       12300|Augusta-Watervill...|       12300|Augusta-Watervill...|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET21DDA63780A7DC09| Oracle Consultants|oracle consultant...|[\\n  \"KS122626T55...|[\\n  \"Procurement...|[\\n  \"KS122626T55...|   [\\n  \"Procurement...|                  []|                  []|                  []|                  []|[\\n  \"BGSBF3F508F...|[\\n  \"Oracle Busi...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|85318b12b3331fa49...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024| 7/7/2024|      35| [\\n  \"Job Board\"\\n]|[\\n  \"dejobs.org\"\\n]|[\\n  \"https://dej...|         []|               NULL|        Data Analyst|Taking care of pe...|      6/10/2024|               8|39063746|            Sedgwick|   Sedgwick|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   5|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 32.77...|    RGFsbGFzLCBUWA==|   Dallas, TX| 48113|    Dallas, TX|19100|Dallas-Fort Worth...|   48|     Texas|          48113|          Dallas, TX|          48113|          Dallas, TX|       19100|Dallas-Fort Worth...|       19100|Dallas-Fort Worth...|    52|Finance and Insur...|   524|Insurance Carrier...|  5242|Agencies, Brokera...| 52429|Other Insurance R...|524291|    Claims Adjusting|ET3037E0C947A02404|      Data Analysts|        data analyst|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"ESF3939CE1F...|   [\\n  \"Exception R...|[\\n  \"KS683TN76T7...|[\\n  \"Security Cl...|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"KS126HY6YLT...|[\\n  \"Microsoft O...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         524|Insurance Carrier...|        5242|Agencies, Brokera...|       52429|Other Insurance R...|      524291|    Claims Adjusting|\n|1b5c3941e54a1889e...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024|7/20/2024|      48| [\\n  \"Job Board\"\\n]|[\\n  \"disabledper...|[\\n  \"https://www...|         []|               NULL|Sr. Lead Data Mgm...|About this role:\\...|      6/12/2024|              10|37615159|         Wells Fargo|Wells Fargo|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (&gt; 32 h...|                   3|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.44...|    UGhvZW5peCwgQVo=|  Phoenix, AZ|  4013|  Maricopa, AZ|38060|Phoenix-Mesa-Chan...|    4|   Arizona|           4013|        Maricopa, AZ|           4013|        Maricopa, AZ|       38060|Phoenix-Mesa-Chan...|       38060|Phoenix-Mesa-Chan...|    52|Finance and Insur...|   522|Credit Intermedia...|  5221|Depository Credit...| 52211|  Commercial Banking|522110|  Commercial Banking|ET2114E0404BA30075|Management Analysts|sr lead data mgmt...|[\\n  \"KS123QX62QY...|[\\n  \"Exit Strate...|[\\n  \"KS123QX62QY...|   [\\n  \"Exit Strate...|                  []|                  []|[\\n  \"KS7G6NP6R6L...|[\\n  \"Reliability...|[\\n  \"KS4409D76NW...|[\\n  \"SAS (Softwa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  6\\n]|  [\\n  \"Data Privac...|          52|Finance and Insur...|         522|Credit Intermedia...|        5221|Depository Credit...|       52211|  Commercial Banking|      522110|  Commercial Banking|\n|cb5ca25f02bdf25c1...|        6/19/2024|   2024-06-19 07:00:00|         0|6/2/2024|6/17/2024|      15|[\\n  \"FreeJobBoar...|[\\n  \"craigslist....|[\\n  \"https://mod...|         []|               NULL|Comisiones de $10...|Comisiones de $10...|      6/17/2024|              15|       0|        Unclassified|      LH/GM|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              3|Part-time / full-...|                NULL|                NULL|        false| 92500|          0|          [None]|               year|   150000|      35000|{\\n  \"lat\": 37.63...|    TW9kZXN0bywgQ0E=|  Modesto, CA|  6099|Stanislaus, CA|33700|         Modesto, CA|    6|California|           6099|      Stanislaus, CA|           6099|      Stanislaus, CA|       33700|         Modesto, CA|       33700|         Modesto, CA|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET0000000000000000|       Unclassified|comisiones de por...|                  []|                  []|                  []|                     []|                  []|                  []|                  []|                  []|                  []|                  []|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The following visualizations are based on the lightcast job postings data frame that was cleaned in the previous section. This analysis explores different facets of the data specifically related to the political affiliation of the states and the different job postings in each state. We also take a closer look at AI related jobs and the impact of political climate on salary."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This code initializes a PySpark environment to load and explore a dataset of job postings. It begins by importing and starting a Spark session named “JobPostingsAnalysis”, then reads a CSV file (lightcast_job_postings.csv) into a Spark DataFrame with headers, schema inference, and support for multi-line fields. The DataFrame is registered as a temporary SQL view called “job_postings” to enable SQL-style queries. Finally, it performs a basic diagnostic check by printing the schema and previewing the first five rows of data—steps that are intended for local debugging and should be commented out when rendering the final submission.\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom pyspark.sql import SparkSession\nimport re\nimport numpy as np\nimport plotly.graph_objects as go\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\njobs_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\njobs_df.createOrReplaceTempView(\"job_postings\")\n\nelections_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/2024_election_results.csv\")\nelections_df.createOrReplaceTempView(\"election_results\")\n\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\n#jobs_df.show(5)\n#elections_df.show(5)"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skills Gap Analysis",
    "section": "",
    "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/15 17:10:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 1:&gt;                                                          (0 + 1) / 1]                                                                                25/10/15 17:10:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\n\n\n\n\n\n\n\n\n\nPython\nJava\nSQL\nPower BI\nMachine Learning\nCloud Computing\n\n\nName\n\n\n\n\n\n\n\n\n\n\nEmily\n2\n2\n2\n4\n1\n2\n\n\nPranathi\n1\n1\n1\n4\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Stage 2:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n\n\n\n\n\n\n\nPython\nJava\nSQL\nPower BI\nMachine Learning\nCloud Computing\n\n\nName\n\n\n\n\n\n\n\n\n\n\nEmily\n2\n2\n2\n4\n1\n2\n\n\nPranathi\n1\n1\n1\n4\n1\n2\n\n\nJob Postings Count\n12350\n4470\n0\n10850\n4324\n2555\n\n\n\n\n\n\n\nBased on our Skills Gap Analysis, we need to work on our Python skills. In order to be more competitive in the job market, we will finish the Python DataCamp and practice incorporating our new skills into our current roles."
  },
  {
    "objectID": "data_cleaning.html#load-the-dataset",
    "href": "data_cleaning.html#load-the-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "This code initializes a PySpark environment to load and explore a dataset of job postings. It begins by importing and starting a Spark session named “JobPostingsAnalysis”, then reads a CSV file (lightcast_job_postings.csv) into a Spark DataFrame with headers, schema inference, and support for multi-line fields. The DataFrame is registered as a temporary SQL view called “job_postings” to enable SQL-style queries. Finally, it performs a basic diagnostic check by printing the schema and previewing the first five rows of data—steps that are intended for local debugging and should be commented out when rendering the final submission.\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom pyspark.sql import SparkSession\nimport re\nimport numpy as np\nimport plotly.graph_objects as go\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\nnp.random.seed(42)\n\npio.renderers.default = \"notebook\"\n\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\njobs_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/lightcast_job_postings.csv\")\njobs_df.createOrReplaceTempView(\"job_postings\")\n\nelections_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"./data/2024_election_results.csv\")\nelections_df.createOrReplaceTempView(\"election_results\")\n\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\n#jobs_df.show(5)\n#elections_df.show(5)"
  },
  {
    "objectID": "eda.html#exploring-the-salary-by-state-political-affiliation",
    "href": "eda.html#exploring-the-salary-by-state-political-affiliation",
    "title": "Exploratory Data Analysis",
    "section": "Exploring the Salary by State Political Affiliation",
    "text": "Exploring the Salary by State Political Affiliation\n\nThis data clearly shows that average salaries are slightly higher in Blue states vs Red States across nearly all NAICS2 categories. In red states, Professional, Scientific, and Technical Services has a slightly higher average salary."
  },
  {
    "objectID": "eda.html#exploring-the-minimum-education-level-by-political-affiliation",
    "href": "eda.html#exploring-the-minimum-education-level-by-political-affiliation",
    "title": "Exploratory Data Analysis",
    "section": "Exploring the minimum education level by political affiliation",
    "text": "Exploring the minimum education level by political affiliation\n\nThis graph shows us the comparison of job postings by political affiliation and minimum education level. As you can see, red states have far more job postings for lower education levels such as High School, Associate Degree, or Bachelor’s degree, and blue states have more postings requiring a Master’s Degree or higher.\n\nThis figure tells us that despite red states having more job postings with education level requirements below a Master’s degree, the salaries in Blue states are higher for every single minimum education level. This could be attributed to a higher cost of living in most blue states."
  },
  {
    "objectID": "eda.html#ai-jobs-analysis",
    "href": "eda.html#ai-jobs-analysis",
    "title": "Exploratory Data Analysis",
    "section": "AI Jobs Analysis",
    "text": "AI Jobs Analysis\n\nThis figure shows us the number of postings just with certain key words related to AI. It groups by education level and political affiliation. The graph tells us that the distribution of number of AI job postings across education level and affiliation mirrors that of the larger data set. Thus, AI jobs are not posted at any higher frequency across red or blue states than any other job.\n\nSimilarly, in this figure, AI Jobs show salary distributions equivalent to that of jobs in other industries. Like the general analysis, blue states offer higher salaries for AI jobs across each of education levels. Again, this is likely due to the higher cost of living in most blue states.\n\nThis graph clearly shows that of the top 5 states with the most number of AI job postings, 4/5 of them are blue states.\n\nThis graph is showing us the number of job postings by NAICS. For AI jobs, the NAICS with the largest number of AI job postings. Computer Systems Design is predominate NAICS with consulting services coming in second.\n\nThe above graph has two different important features. First, it shows the top 10 industries for women on the bar chart, sorted by the overall number of women according the U.S. Bureau of Labor Statistics. The second important feature is the line chart which shows the percentage of women in that field. As you can see, some of the most female dominated industries do not have very many women in them and some of the industries with the most women are heavily male dominated.\n\nThe above figure shows the top 10 industries with the most number of employed individuals based on data from the U.S. Bureau of Labor Statistics. The blue bars show the number of people in those industries, and the red bars show the number of women in the industry."
  },
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "ML Methods",
    "section": "",
    "text": "Show code\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set Plotly theme\npio.templates.default = \"plotly_white\"\n\nprint(\"✓ All libraries loaded successfully!\")\n\n\n✓ All libraries loaded successfully!"
  },
  {
    "objectID": "ml_methods.html#data-preprocessing",
    "href": "ml_methods.html#data-preprocessing",
    "title": "ML Methods",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\n\nShow code\n# Create working copy\ndf_clean = df.copy()\n\n# Remove rows with missing salary or state\nrequired_cols = ['SALARY', 'STATE']\ninitial_rows = len(df_clean)\ndf_clean = df_clean.dropna(subset=required_cols)\nremoved_rows = initial_rows - len(df_clean)\n\nprint(f\"Removed {removed_rows:,} rows with missing salary or state data\")\nprint(f\"Final dataset: {len(df_clean):,} rows\")\n\n\nRemoved 41,690 rows with missing salary or state data\nFinal dataset: 30,808 rows\n\n\n\n\nShow code\n## Political Leaning Classification (Fixed)\n\n#| label: fix-fips-and-political-leaning\n#| code-fold: true\n\n# FIPS code to state abbreviation mapping\nfips_to_state = {\n    1: 'AL', 2: 'AK', 4: 'AZ', 5: 'AR', 6: 'CA', 8: 'CO', 9: 'CT', 10: 'DE',\n    11: 'DC', 12: 'FL', 13: 'GA', 15: 'HI', 16: 'ID', 17: 'IL', 18: 'IN',\n    19: 'IA', 20: 'KS', 21: 'KY', 22: 'LA', 23: 'ME', 24: 'MD', 25: 'MA',\n    26: 'MI', 27: 'MN', 28: 'MS', 29: 'MO', 30: 'MT', 31: 'NE', 32: 'NV',\n    33: 'NH', 34: 'NJ', 35: 'NM', 36: 'NY', 37: 'NC', 38: 'ND', 39: 'OH',\n    40: 'OK', 41: 'OR', 42: 'PA', 44: 'RI', 45: 'SC', 46: 'SD', 47: 'TN',\n    48: 'TX', 49: 'UT', 50: 'VT', 51: 'VA', 53: 'WA', 54: 'WV', 55: 'WI',\n    56: 'WY', 72: 'PR'\n}\n\n# Convert FIPS codes to state abbreviations\ndf_clean['STATE_ABBREV'] = df_clean['STATE'].apply(\n    lambda x: fips_to_state.get(int(x), 'Unknown') if pd.notna(x) else 'Unknown'\n)\n\n# Political classifications\nred_states = ['AL', 'AK', 'AR', 'FL', 'ID', 'IN', 'IA', 'KS', 'KY', \n              'LA', 'MS', 'MO', 'MT', 'NE', 'ND', 'OH', 'OK', 'SC', \n              'SD', 'TN', 'TX', 'UT', 'WV', 'WY']\n\nblue_states = ['CA', 'CO', 'CT', 'DE', 'HI', 'IL', 'ME', 'MD', 'MA', \n               'MI', 'MN', 'NH', 'NJ', 'NM', 'NY', 'OR', 'PA', 'RI', \n               'VT', 'VA', 'WA', 'WI', 'DC']\n\nswing_states = ['AZ', 'GA', 'NC', 'NV']\n\ndef assign_political_leaning(state_abbrev):\n    if pd.isna(state_abbrev) or state_abbrev == 'Unknown':\n        return 'Unknown'\n    state_abbrev = str(state_abbrev).upper()\n    if state_abbrev in red_states:\n        return 'Red'\n    elif state_abbrev in blue_states:\n        return 'Blue'\n    elif state_abbrev in swing_states:\n        return 'Swing'\n    else:\n        return 'Other'\n\ndf_clean['political_leaning'] = df_clean['STATE_ABBREV'].apply(assign_political_leaning)\n\nprint(\"=\"*60)\nprint(\"POLITICAL LEANING DISTRIBUTION (FIXED)\")\nprint(\"=\"*60)\nprint(df_clean['political_leaning'].value_counts())\nprint(\"\\nPercentage:\")\nprint((df_clean['political_leaning'].value_counts() / len(df_clean) * 100).round(2))\n\n\n============================================================\nPOLITICAL LEANING DISTRIBUTION (FIXED)\n============================================================\npolitical_leaning\nBlue     17798\nRed      10212\nSwing     2798\nName: count, dtype: int64\n\nPercentage:\npolitical_leaning\nBlue     57.77\nRed      33.15\nSwing     9.08\nName: count, dtype: float64\n\n\n\n\nShow code\n# Visualize political leaning distribution\nfig = px.pie(\n    values=df_clean['political_leaning'].value_counts().values,\n    names=df_clean['political_leaning'].value_counts().index,\n    title='Distribution of Jobs by Political Leaning of State',\n    hole=0.4,\n    color_discrete_map={'Red': '#FF6B6B', 'Blue': '#4ECDC4', 'Swing': '#FFD93D', 'Other': '#95A5A6'}\n)\nfig.update_layout(template=\"plotly_white\", height=400)\nfig.show()"
  },
  {
    "objectID": "ml_methods.html#k-means-clustering-unsupervised",
    "href": "ml_methods.html#k-means-clustering-unsupervised",
    "title": "ML Methods",
    "section": "K-Means Clustering (unsupervised)",
    "text": "K-Means Clustering (unsupervised)\n\nSetup and Feature Engineering\n\n\nShow code\n# Determine which reference label to use (SOC, NAICS, or ONET)\nreference_label = None\nfor label in ['SOC_2', 'NAICS_2022_2', 'ONET', 'LIGHTCAST_SECTORS']:\n    if label in df_clean.columns and df_clean[label].notna().sum() &gt; 0:\n        reference_label = label\n        print(f\"✓ Using {label} as reference label\")\n        break\n\nif reference_label is None:\n    print(\"No classification column found. Using TITLE as reference.\")\n    reference_label = 'TITLE'\n\nprint(f\"\\nReference Label: {reference_label}\")\nprint(f\"Unique values: {df_clean[reference_label].nunique():,}\")\nprint(f\"\\nTop 10 {reference_label} categories:\")\nprint(df_clean[reference_label].value_counts().head(10))\n\n\n✓ Using SOC_2 as reference label\n\nReference Label: SOC_2\nUnique values: 1\n\nTop 10 SOC_2 categories:\nSOC_2\n15-0000    30808\nName: count, dtype: int64\n\n\n\n\nView feature engineering code\n# Prepare features for clustering\ndf_cluster = df_clean.copy()\n\nprint(\"=\"*80)\nprint(\"FEATURE ENGINEERING FOR CLUSTERING\")\nprint(\"=\"*80)\n\n# Encode categorical variables\nencoders = {}\ncategorical_cols = ['STATE', 'TITLE', 'political_leaning', 'LIGHTCAST_SECTORS']\n\nprint(f\"\\nEncoding categorical variables:\")\nfor col in categorical_cols:\n    if col in df_cluster.columns:\n        le = LabelEncoder()\n        df_cluster[f'{col}_encoded'] = le.fit_transform(\n            df_cluster[col].fillna('Unknown').astype(str)\n        )\n        encoders[col] = le\n        print(f\"  ✓ {col}: {df_cluster[col].nunique()} unique values\")\n\n# Select clustering features\nclustering_features = ['SALARY']\n\nfor col in categorical_cols:\n    encoded_col = f'{col}_encoded'\n    if encoded_col in df_cluster.columns:\n        clustering_features.append(encoded_col)\n\n# Add years of experience if available\nif 'MIN_YEARS_EXPERIENCE' in df_cluster.columns:\n    df_cluster['MIN_YEARS_EXPERIENCE'] = pd.to_numeric(\n        df_cluster['MIN_YEARS_EXPERIENCE'], errors='coerce'\n    )\n    clustering_features.append('MIN_YEARS_EXPERIENCE')\n    print(f\"  ✓ MIN_YEARS_EXPERIENCE: numeric feature\")\n\nprint(f\"\\n📊 Total Clustering Features: {len(clustering_features)}\")\nprint(\"\\nFeature List:\")\nfor i, feature in enumerate(clustering_features, 1):\n    print(f\"  {i}. {feature}\")\n\n# Prepare feature matrix\nX_cluster = df_cluster[clustering_features].fillna(df_cluster[clustering_features].mean())\nprint(f\"\\nFeature Matrix Shape: {X_cluster.shape}\")\n\n\n================================================================================\nFEATURE ENGINEERING FOR CLUSTERING\n================================================================================\n\nEncoding categorical variables:\n  ✓ STATE: 51 unique values\n  ✓ TITLE: 3286 unique values\n  ✓ political_leaning: 3 unique values\n  ✓ LIGHTCAST_SECTORS: 23 unique values\n  ✓ MIN_YEARS_EXPERIENCE: numeric feature\n\n📊 Total Clustering Features: 6\n\nFeature List:\n  1. SALARY\n  2. STATE_encoded\n  3. TITLE_encoded\n  4. political_leaning_encoded\n  5. LIGHTCAST_SECTORS_encoded\n  6. MIN_YEARS_EXPERIENCE\n\nFeature Matrix Shape: (30808, 6)\n\n\n\n\nDetermine Optimal K\n\n\nView elbow method code\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\n# Test different numbers of clusters\nK_range = range(2, 11)\ninertias = []\nsilhouette_scores = []\n\nprint(\"Testing different numbers of clusters...\")\nprint(f\"{'k':&lt;5} {'Inertia':&lt;15} {'Silhouette Score'}\")\nprint(\"-\" * 40)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n    silhouette_scores.append(sil_score)\n    print(f\"{k:&lt;5} {kmeans.inertia_:&lt;15.2f} {sil_score:.4f}\")\n\noptimal_k = list(K_range)[silhouette_scores.index(max(silhouette_scores))]\nprint(f\"\\n💡 Optimal k based on Silhouette Score: {optimal_k}\")\n\n\nTesting different numbers of clusters...\nk     Inertia         Silhouette Score\n----------------------------------------\n2     154056.48       0.2204\n3     131383.22       0.1936\n4     115316.04       0.1890\n5     103986.36       0.1827\n6     97630.31        0.1782\n7     92195.28        0.1813\n8     86787.56        0.1753\n9     82257.94        0.1792\n10    78108.55        0.1882\n\n💡 Optimal k based on Silhouette Score: 2\n\n\n\n\nShow code\n# Visualize elbow curve and silhouette scores\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=('Elbow Method', 'Silhouette Score Method')\n)\n\nfig.add_trace(\n    go.Scatter(x=list(K_range), y=inertias, mode='lines+markers', \n               name='Inertia', line=dict(color='blue')),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=list(K_range), y=silhouette_scores, mode='lines+markers', \n               name='Silhouette', line=dict(color='orange')),\n    row=1, col=2\n)\n\nfig.update_xaxes(title_text=\"Number of Clusters (k)\", row=1, col=1)\nfig.update_xaxes(title_text=\"Number of Clusters (k)\", row=1, col=2)\nfig.update_yaxes(title_text=\"Inertia\", row=1, col=1)\nfig.update_yaxes(title_text=\"Silhouette Score\", row=1, col=2)\n\nfig.update_layout(\n    height=400, \n    showlegend=False, \n    template=\"plotly_white\",\n    title_text=\"Determining Optimal Number of Clusters\"\n)\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nCluster Analysis\n\n\nShow code\n# Perform KMeans with k=5 (per assignment requirements)\nn_clusters = 5\n\nprint(f\"Performing KMeans with k={n_clusters} clusters...\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\ndf_cluster['cluster'] = kmeans.fit_predict(X_scaled)\n\nprint(f\"✓ Clustering complete!\")\nprint(f\"\\nCluster Distribution:\")\ncluster_counts = df_cluster['cluster'].value_counts().sort_index()\nfor cluster_id, count in cluster_counts.items():\n    pct = (count / len(df_cluster)) * 100\n    print(f\"  Cluster {cluster_id}: {count:,} jobs ({pct:.1f}%)\")\n\n\nPerforming KMeans with k=5 clusters...\n✓ Clustering complete!\n\nCluster Distribution:\n  Cluster 0: 8,137 jobs (26.4%)\n  Cluster 1: 5,007 jobs (16.3%)\n  Cluster 2: 5,208 jobs (16.9%)\n  Cluster 3: 6,211 jobs (20.2%)\n  Cluster 4: 6,245 jobs (20.3%)\n\n\n\n\nShow code\n# Sample for performance (5000 points)\nsample_size = min(5000, len(df_cluster))\ndf_sample = df_cluster.sample(sample_size, random_state=42)\n\nfig = px.scatter(\n    df_sample,\n    x='SALARY',\n    y='STATE_encoded',\n    color='cluster',\n    hover_data=['TITLE', 'political_leaning', reference_label] if 'TITLE' in df_sample.columns else None,\n    title=f'KMeans Clustering Results (k={n_clusters}, n={sample_size:,} sample)',\n    labels={'SALARY': 'Annual Salary ($)', 'STATE_encoded': 'State (Encoded)', 'cluster': 'Cluster'},\n    color_continuous_scale='Viridis'\n)\nfig.update_layout(template=\"plotly_white\", height=550, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\n\n                            \n                                            \n\n\nKey Findings: - Cluster 0: Entry-level positions ($50-100k) - Cluster 4: Senior roles ($200k+) - Geographic clustering evident by state encoding\n\n\nShow code\n# Analyze cluster characteristics\nprint(\"=\"*80)\nprint(\"CLUSTER PROFILES\")\nprint(\"=\"*80)\n\ncluster_profiles = df_cluster.groupby('cluster').agg({\n    'SALARY': ['mean', 'median', 'std', 'min', 'max'],\n    'political_leaning': lambda x: x.mode()[0] if len(x.mode()) &gt; 0 else 'Mixed',\n    'cluster': 'count'\n})\n\ncluster_profiles.columns = [\n    'Avg_Salary', 'Median_Salary', 'Salary_StdDev', 'Min_Salary', 'Max_Salary',\n    'Dominant_Political', 'Count'\n]\n\ncluster_profiles = cluster_profiles.round(2)\nprint(cluster_profiles)\n\n\n================================================================================\nCLUSTER PROFILES\n================================================================================\n         Avg_Salary  Median_Salary  Salary_StdDev  Min_Salary  Max_Salary  \\\ncluster                                                                     \n0          97442.86        95000.0       33748.50     21237.0    250000.0   \n1         170814.08       166500.0       42553.06     77050.0    500000.0   \n2         103140.74       100000.0       39435.72     15860.0    264976.0   \n3         111217.04       110000.0       39249.52     20800.0    275000.0   \n4         121350.62       119300.0       35456.33     23585.0    305000.0   \n\n        Dominant_Political  Count  \ncluster                            \n0                     Blue   8137  \n1                     Blue   5007  \n2                      Red   5208  \n3                     Blue   6211  \n4                     Blue   6245  \n\n\n\n\nShow code\n# Visualize cluster salary profiles\nprofile_df = cluster_profiles.reset_index()\n\nfig = px.bar(\n    profile_df,\n    x='cluster',\n    y='Avg_Salary',\n    text='Count',\n    title='Average Salary by Cluster',\n    labels={'cluster': 'Cluster', 'Avg_Salary': 'Average Salary ($)'},\n    color='Avg_Salary',\n    color_continuous_scale='Viridis'\n)\nfig.update_traces(texttemplate='n=%{text:,}', textposition='outside')\nfig.update_layout(template=\"plotly_white\", height=450, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nShow code\n# Compare clusters with reference labels\nprint(f\"\\n{'='*80}\")\nprint(f\"CLUSTER vs {reference_label.upper()} COMPARISON\")\nprint(f\"{'='*80}\")\n\nfor cluster_id in sorted(df_cluster['cluster'].unique()):\n    cluster_data = df_cluster[df_cluster['cluster'] == cluster_id]\n    top_categories = cluster_data[reference_label].value_counts().head(5)\n    \n    print(f\"\\n📊 Cluster {cluster_id}:\")\n    print(f\"   Size: {len(cluster_data):,} jobs\")\n    print(f\"   Avg Salary: ${cluster_data['SALARY'].mean():,.2f}\")\n    if 'political_leaning' in cluster_data.columns:\n        print(f\"   Dominant Political: {cluster_data['political_leaning'].mode()[0] if len(cluster_data['political_leaning'].mode()) &gt; 0 else 'Mixed'}\")\n    print(f\"   Top 5 {reference_label} categories:\")\n    for category, count in top_categories.items():\n        pct = (count / len(cluster_data)) * 100\n        print(f\"     • {category}: {count:,} ({pct:.1f}%)\")\n\n\n\n================================================================================\nCLUSTER vs SOC_2 COMPARISON\n================================================================================\n\n📊 Cluster 0:\n   Size: 8,137 jobs\n   Avg Salary: $97,442.86\n   Dominant Political: Blue\n   Top 5 SOC_2 categories:\n     • 15-0000: 8,137 (100.0%)\n\n📊 Cluster 1:\n   Size: 5,007 jobs\n   Avg Salary: $170,814.08\n   Dominant Political: Blue\n   Top 5 SOC_2 categories:\n     • 15-0000: 5,007 (100.0%)\n\n📊 Cluster 2:\n   Size: 5,208 jobs\n   Avg Salary: $103,140.74\n   Dominant Political: Red\n   Top 5 SOC_2 categories:\n     • 15-0000: 5,208 (100.0%)\n\n📊 Cluster 3:\n   Size: 6,211 jobs\n   Avg Salary: $111,217.04\n   Dominant Political: Blue\n   Top 5 SOC_2 categories:\n     • 15-0000: 6,211 (100.0%)\n\n📊 Cluster 4:\n   Size: 6,245 jobs\n   Avg Salary: $121,350.62\n   Dominant Political: Blue\n   Top 5 SOC_2 categories:\n     • 15-0000: 6,245 (100.0%)"
  },
  {
    "objectID": "ml_methods.html#data-loading-and-exploration",
    "href": "ml_methods.html#data-loading-and-exploration",
    "title": "ML Methods",
    "section": "Data Loading and Exploration",
    "text": "Data Loading and Exploration\n\n\nShow code\n# Load lightcast job postings data\ndf = pd.read_csv('data/lightcast_job_postings.csv')\n\nprint(f\"Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\nprint(f\"\\nFirst few rows:\")\ndf.head()\n\n\nDataset Shape: 72,498 rows × 131 columns\n\nFirst few rows:\n\n\n\n\n\n\n\n\n\nID\nLAST_UPDATED_DATE\nLAST_UPDATED_TIMESTAMP\nDUPLICATES\nPOSTED\nEXPIRED\nDURATION\nSOURCE_TYPES\nSOURCES\nURL\n...\nNAICS_2022_2\nNAICS_2022_2_NAME\nNAICS_2022_3\nNAICS_2022_3_NAME\nNAICS_2022_4\nNAICS_2022_4_NAME\nNAICS_2022_5\nNAICS_2022_5_NAME\nNAICS_2022_6\nNAICS_2022_6_NAME\n\n\n\n\n0\n1f57d95acf4dc67ed2819eb12f049f6a5c11782c\n9/6/2024\n2024-09-06 20:32:57.352 Z\n0.0\n6/2/2024\n6/8/2024\n6.0\n[\\n \"Company\"\\n]\n[\\n \"brassring.com\"\\n]\n[\\n \"https://sjobs.brassring.com/TGnewUI/Sear...\n...\n44.0\nRetail Trade\n441.0\nMotor Vehicle and Parts Dealers\n4413.0\nAutomotive Parts, Accessories, and Tire Retailers\n44133.0\nAutomotive Parts and Accessories Retailers\n441330.0\nAutomotive Parts and Accessories Retailers\n\n\n1\n0cb072af26757b6c4ea9464472a50a443af681ac\n8/2/2024\n2024-08-02 17:08:58.838 Z\n0.0\n6/2/2024\n8/1/2024\nNaN\n[\\n \"Job Board\"\\n]\n[\\n \"maine.gov\"\\n]\n[\\n \"https://joblink.maine.gov/jobs/1085740\"\\n]\n...\n56.0\nAdministrative and Support and Waste Managemen...\n561.0\nAdministrative and Support Services\n5613.0\nEmployment Services\n56132.0\nTemporary Help Services\n561320.0\nTemporary Help Services\n\n\n2\n85318b12b3331fa490d32ad014379df01855c557\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/7/2024\n35.0\n[\\n \"Job Board\"\\n]\n[\\n \"dejobs.org\"\\n]\n[\\n \"https://dejobs.org/dallas-tx/data-analys...\n...\n52.0\nFinance and Insurance\n524.0\nInsurance Carriers and Related Activities\n5242.0\nAgencies, Brokerages, and Other Insurance Rela...\n52429.0\nOther Insurance Related Activities\n524291.0\nClaims Adjusting\n\n\n3\n1b5c3941e54a1889ef4f8ae55b401a550708a310\n9/6/2024\n2024-09-06 20:32:57.352 Z\n1.0\n6/2/2024\n7/20/2024\n48.0\n[\\n \"Job Board\"\\n]\n[\\n \"disabledperson.com\",\\n \"dejobs.org\"\\n]\n[\\n \"https://www.disabledperson.com/jobs/5948...\n...\n52.0\nFinance and Insurance\n522.0\nCredit Intermediation and Related Activities\n5221.0\nDepository Credit Intermediation\n52211.0\nCommercial Banking\n522110.0\nCommercial Banking\n\n\n4\ncb5ca25f02bdf25c13edfede7931508bfd9e858f\n6/19/2024\n2024-06-19 07:00:00.000 Z\n0.0\n6/2/2024\n6/17/2024\n15.0\n[\\n \"FreeJobBoard\"\\n]\n[\\n \"craigslist.org\"\\n]\n[\\n \"https://modesto.craigslist.org/sls/77475...\n...\n99.0\nUnclassified Industry\n999.0\nUnclassified Industry\n9999.0\nUnclassified Industry\n99999.0\nUnclassified Industry\n999999.0\nUnclassified Industry\n\n\n\n\n5 rows × 131 columns\n\n\n\n\n\nShow code\n# Check data quality\nprint(\"=\"*80)\nprint(\"DATA QUALITY ASSESSMENT\")\nprint(\"=\"*80)\n\n# Key columns for analysis\nkey_columns = ['SALARY', 'STATE', 'TITLE', 'NAICS_2022_2', 'SOC_2', 'ONET', 'LIGHTCAST_SECTORS']\n\ninfo_df = pd.DataFrame({\n    'Column': key_columns,\n    'Missing': [df[col].isnull().sum() if col in df.columns else 'N/A' for col in key_columns],\n    'Missing %': [f\"{(df[col].isnull().sum() / len(df) * 100):.2f}%\" if col in df.columns else 'N/A' for col in key_columns],\n    'Unique Values': [df[col].nunique() if col in df.columns else 'N/A' for col in key_columns]\n})\n\nprint(info_df.to_string(index=False))\n\n# Salary statistics\nif 'SALARY' in df.columns:\n    print(f\"\\nSalary Statistics:\")\n    print(f\"  Mean: ${df['SALARY'].mean():,.2f}\")\n    print(f\"  Median: ${df['SALARY'].median():,.2f}\")\n    print(f\"  Std Dev: ${df['SALARY'].std():,.2f}\")\n    print(f\"  Range: ${df['SALARY'].min():,.2f} - ${df['SALARY'].max():,.2f}\")\n\n\n================================================================================\nDATA QUALITY ASSESSMENT\n================================================================================\n           Column  Missing Missing %  Unique Values\n           SALARY    41690    57.51%           6052\n            STATE       44     0.06%             51\n            TITLE       44     0.06%           5719\n     NAICS_2022_2       44     0.06%             21\n            SOC_2       44     0.06%              1\n             ONET       44     0.06%              1\nLIGHTCAST_SECTORS    54711    75.47%             23\n\nSalary Statistics:\n  Mean: $117,953.76\n  Median: $116,300.00\n  Std Dev: $45,133.88\n  Range: $15,860.00 - $500,000.00"
  },
  {
    "objectID": "ml_methods.html#multiple-regression-analysis",
    "href": "ml_methods.html#multiple-regression-analysis",
    "title": "ML Methods",
    "section": "Multiple Regression Analysis",
<<<<<<< HEAD
    "text": "Multiple Regression Analysis\n\n\nView feature engineering code\n# Prepare features for salary prediction\ndf_reg = df_clean.copy()\n\nprint(\"=\"*80)\nprint(\"FEATURE ENGINEERING FOR SALARY PREDICTION\")\nprint(\"=\"*80)\n\n# Encode categorical variables\nle_reg = {}\ncategorical_features = ['STATE', 'TITLE', 'political_leaning', 'LIGHTCAST_SECTORS']\n\n# Add SOC or NAICS if available\nif 'SOC_2' in df_reg.columns:\n    categorical_features.append('SOC_2')\nelif 'NAICS_2022_2' in df_reg.columns:\n    categorical_features.append('NAICS_2022_2')\n\nprint(f\"\\nCategorical features to encode ({len(categorical_features)}):\")\nfor col in categorical_features:\n    if col in df_reg.columns:\n        le = LabelEncoder()\n        df_reg[f'{col}_encoded'] = le.fit_transform(\n            df_reg[col].fillna('Unknown').astype(str)\n        )\n        le_reg[col] = le\n        print(f\"  ✓ {col}: {df_reg[col].nunique()} unique values\")\n\n# Select features for regression\nfeature_cols = []\nfor col in categorical_features:\n    encoded_col = f'{col}_encoded'\n    if encoded_col in df_reg.columns:\n        feature_cols.append(encoded_col)\n\n# Add numerical features if available\nnumeric_features = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']\nfor num_feat in numeric_features:\n    if num_feat in df_reg.columns:\n        df_reg[num_feat] = pd.to_numeric(df_reg[num_feat], errors='coerce')\n        if df_reg[num_feat].notna().sum() &gt; 0:\n            feature_cols.append(num_feat)\n            print(f\"  ✓ {num_feat}: numeric feature added\")\n\nprint(f\"\\n Total Features for Salary Prediction: {len(feature_cols)}\")\n\n# Prepare X and y\nX = df_reg[feature_cols].fillna(df_reg[feature_cols].mean())\ny = df_reg['SALARY']\n\nprint(f\"\\n Dataset Statistics:\")\nprint(f\"  • Feature Matrix Shape: {X.shape}\")\nprint(f\"  • Salary Statistics:\")\nprint(f\"    - Mean: ${y.mean():,.2f}\")\nprint(f\"    - Median: ${y.median():,.2f}\")\nprint(f\"    - Std Dev: ${y.std():,.2f}\")\nprint(f\"    - Range: ${y.min():,.2f} - ${y.max():,.2f}\")\n\n\nFeature Selection Justification: The features were selected based on their theoretical and empirical relationship with salary:\nSTATE & Political Leaning: Geographic location and political climate influence cost of living and compensation policies TITLE: Job title is the primary indicator of role level and responsibility LIGHTCAST_SECTORS: Industry sector determines baseline compensation structure SOC/NAICS: Occupation classification provides standardized job categorization Years of Experience: Direct correlation with salary progression (if available)\n\n\nShow code\n# Split data (70/30 as per assignment requirements)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42\n)\n\nprint(\"=\"*80)\nprint(\"TRAIN/TEST SPLIT\")\nprint(\"=\"*80)\nprint(f\"\\nTraining Set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Test Set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"\\nSalary Distribution:\")\nprint(f\"  Training - Mean: ${y_train.mean():,.2f}, Std: ${y_train.std():,.2f}\")\nprint(f\"  Test - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}\")\n\n\n\n\nView Linear Regression code\nprint(\"=\"*80)\nprint(\"MODEL 1: MULTIPLE LINEAR REGRESSION\")\nprint(\"=\"*80)\n\n# Train model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Make predictions\ny_pred_lin = lin_reg.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))\nr2_lin = r2_score(y_test, y_pred_lin)\nmae_lin = np.mean(np.abs(y_test - y_pred_lin))\n\nprint(f\"\\n📊 Model Performance:\")\nprint(f\"   • R² Score: {r2_lin:.4f}\")\nprint(f\"     → Explains {r2_lin*100:.2f}% of salary variance\")\nprint(f\"   • RMSE: ${rmse_lin:,.2f}\")\nprint(f\"   • MAE: ${mae_lin:,.2f}\")\n\n# Feature coefficients\ncoef_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Coefficient': lin_reg.coef_\n}).sort_values('Coefficient', key=abs, ascending=False)\n\nprint(f\"\\n📈 Top 10 Most Influential Features:\")\nprint(coef_df.head(10).to_string(index=False))\n\n\n\n\nShow code\n# Visualize top 15 feature coefficients\ntop_features = coef_df.head(15)\n\nfig = px.bar(\n    top_features,\n    x='Coefficient',\n    y='Feature',\n    orientation='h',\n    title='Top 15 Feature Coefficients - Multiple Linear Regression',\n    labels={'Coefficient': 'Impact on Salary ($)', 'Feature': 'Feature'},\n    color='Coefficient',\n    color_continuous_scale='RdBu',\n    color_continuous_midpoint=0\n)\n\nfig.update_layout(template=\"plotly_white\", height=500, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\n\n\n\nView Random Forest code\nprint(\"=\"*80)\nprint(\"MODEL 2: RANDOM FOREST REGRESSION\")\nprint(\"=\"*80)\n\n# Train model\nrf_reg = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=20,\n    min_samples_split=10,\n    min_samples_leaf=4,\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"Training Random Forest model...\")\nrf_reg.fit(X_train, y_train)\nprint(\"✓ Training complete!\")\n\n# Make predictions\ny_pred_rf = rf_reg.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nr2_rf = r2_score(y_test, y_pred_rf)\nmae_rf = np.mean(np.abs(y_test - y_pred_rf))\n\nprint(f\"\\n Model Performance:\")\nprint(f\"   • R² Score: {r2_rf:.4f}\")\nprint(f\"     → Explains {r2_rf*100:.2f}% of salary variance\")\nprint(f\"   • RMSE: ${rmse_rf:,.2f}\")\nprint(f\"   • MAE: ${mae_rf:,.2f}\")\n\n# Feature importance\nimportance_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Importance': rf_reg.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(f\"\\n Top 10 Most Important Features:\")\nprint(importance_df.head(10).to_string(index=False))\n\n# Calculate improvement safely\nif r2_lin &gt; 0:\n    improvement = ((r2_rf - r2_lin) / r2_lin) * 100\n    print(f\"\\n🚀 Random Forest improves R² by {improvement:.1f}% over Linear Regression\")\n\n\n\n\nShow code\n# Visualize feature importance\ntop_features_rf = importance_df.head(15)\n\nfig = px.bar(\n    top_features_rf,\n    x='Importance',\n    y='Feature',\n    orientation='h',\n    title='Top 15 Feature Importance - Random Forest Regression',\n    labels={'Importance': 'Importance Score', 'Feature': 'Feature'},\n    color='Importance',\n    color_continuous_scale='Viridis'\n)\n\nfig.update_layout(template=\"plotly_white\", height=500, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\n\n\n\nShow code\nprint(\"=\"*80)\nprint(\"REGRESSION MODEL COMPARISON\")\nprint(\"=\"*80)\n\n# Create comparison dataframe\ncomparison_df = pd.DataFrame({\n    'Model': ['Multiple Linear Regression', 'Random Forest Regression'],\n    'R² Score': [r2_lin, r2_rf],\n    'RMSE ($)': [rmse_lin, rmse_rf],\n    'MAE ($)': [mae_lin, mae_rf]\n})\n\nprint(\"\\n\" + comparison_df.to_string(index=False))\n\nbest_model = comparison_df.loc[comparison_df['R² Score'].idxmax(), 'Model']\nbest_r2 = comparison_df['R² Score'].max()\n\nprint(f\"\\n🏆 Best Performing Model: {best_model}\")\nprint(f\"   • Achieves R² of {best_r2:.4f}\")\nprint(f\"   • Explains {best_r2*100:.2f}% of salary variance\")\n\n\n\n\nShow code\n# Sample for performance\nsample_size = min(2000, len(y_test))\nindices = np.random.choice(len(y_test), sample_size, replace=False)\n\ncomparison_results = pd.DataFrame({\n    'Actual': y_test.iloc[indices],\n    'Linear_Regression': y_pred_lin[indices],\n    'Random_Forest': y_pred_rf[indices]\n})\n\nfig = go.Figure()\n\n# Random Forest predictions\nfig.add_trace(go.Scatter(\n    x=comparison_results['Actual'],\n    y=comparison_results['Random_Forest'],\n    mode='markers',\n    name='Random Forest',\n    opacity=0.6,\n    marker=dict(size=5, color='blue')\n))\n\n# Linear Regression predictions\nfig.add_trace(go.Scatter(\n    x=comparison_results['Actual'],\n    y=comparison_results['Linear_Regression'],\n    mode='markers',\n    name='Linear Regression',\n    opacity=0.6,\n    marker=dict(size=5, color='red')\n))\n\n# Perfect prediction line\nmin_val = comparison_results['Actual'].min()\nmax_val = comparison_results['Actual'].max()\n\nfig.add_trace(go.Scatter(\n    x=[min_val, max_val],\n    y=[min_val, max_val],\n    mode='lines',\n    name='Perfect Prediction',\n    line=dict(color='green', dash='dash', width=2)\n))\n\nfig.update_layout(\n    title=f'Actual vs Predicted Salary - Model Comparison (n={sample_size:,})',\n    xaxis_title='Actual Salary ($)',\n    yaxis_title='Predicted Salary ($)',\n    template=\"plotly_white\",\n    height=550,\n    hovermode='closest'\n)\n\nfig.show()"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Gender Disparities in Hiring & Political Influence (U.S. and Global, 2024–2025)",
    "section": "",
    "text": "We analyze gender disparities in hiring, AI participation, and wages across U.S. industries and states, and situate these patterns in a global context. Drawing on recent federal statistics ( (Anon. (2025a))), U.S. pay-gap trackers (American Association of University Women ((2025a));  (Anon. (2024a))), and international reports ( (Anon. (2024b));  (Anon. (2023));  (Anon. (2025b));  (Anon. (2025c))), we find persistent occupational segregation, widening U.S. annual earnings gaps since 2022, and continued underrepresentation of women in AI-intensive roles. State-level disparities correlate with policy environments such as pay-transparency and salary-history bans (American Association of University Women ((2025b))), though industry composition and family structure are important confounders. Conservative/market-based perspectives emphasize occupational choice, hours, and career continuity as key mechanisms and warn that some transparency policies may compress wages (American Enterprise Institute ((2021)); Heritage Foundation ((2024)); Cullen and Pakzad-Hurson ((2021)); Cullen ((2023)); Mas ((2014))). We integrate both views and outline implications for job seekers selecting sectors, geographies, and employers."
  },
  {
    "objectID": "introduction.html#embedded-visualization-aauw-live-state-map",
    "href": "introduction.html#embedded-visualization-aauw-live-state-map",
    "title": "Gender Disparities in Hiring & Political Influence (U.S. and Global, 2024–2025)",
    "section": "6.1 Embedded visualization: AAUW live state map",
    "text": "6.1 Embedded visualization: AAUW live state map\n\n\n\n\n\n\nNote\n\n\n\nTip: The live embed is perfect for your qualitative section. For PDF exports, also include a static image below as fallback.\n\n\n\n\n\n\nSource: American Association of University Women ((2025b))."
=======
    "text": "Multiple Regression Analysis\n\n\nView feature engineering code\n# Prepare features for salary prediction\ndf_reg = df_clean.copy()\n\nprint(\"=\"*80)\nprint(\"FEATURE ENGINEERING FOR SALARY PREDICTION\")\nprint(\"=\"*80)\n\n# Encode categorical variables\nle_reg = {}\ncategorical_features = ['STATE', 'TITLE', 'political_leaning', 'LIGHTCAST_SECTORS']\n\n# Add SOC or NAICS if available\nif 'SOC_2' in df_reg.columns:\n    categorical_features.append('SOC_2')\nelif 'NAICS_2022_2' in df_reg.columns:\n    categorical_features.append('NAICS_2022_2')\n\nprint(f\"\\nCategorical features to encode ({len(categorical_features)}):\")\nfor col in categorical_features:\n    if col in df_reg.columns:\n        le = LabelEncoder()\n        df_reg[f'{col}_encoded'] = le.fit_transform(\n            df_reg[col].fillna('Unknown').astype(str)\n        )\n        le_reg[col] = le\n        print(f\"  ✓ {col}: {df_reg[col].nunique()} unique values\")\n\n# Select features for regression\nfeature_cols = []\nfor col in categorical_features:\n    encoded_col = f'{col}_encoded'\n    if encoded_col in df_reg.columns:\n        feature_cols.append(encoded_col)\n\n# Add numerical features if available\nnumeric_features = ['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']\nfor num_feat in numeric_features:\n    if num_feat in df_reg.columns:\n        df_reg[num_feat] = pd.to_numeric(df_reg[num_feat], errors='coerce')\n        if df_reg[num_feat].notna().sum() &gt; 0:\n            feature_cols.append(num_feat)\n            print(f\"  ✓ {num_feat}: numeric feature added\")\n\nprint(f\"\\n Total Features for Salary Prediction: {len(feature_cols)}\")\n\n# Prepare X and y\nX = df_reg[feature_cols].fillna(df_reg[feature_cols].mean())\ny = df_reg['SALARY']\n\nprint(f\"\\n Dataset Statistics:\")\nprint(f\"  • Feature Matrix Shape: {X.shape}\")\nprint(f\"  • Salary Statistics:\")\nprint(f\"    - Mean: ${y.mean():,.2f}\")\nprint(f\"    - Median: ${y.median():,.2f}\")\nprint(f\"    - Std Dev: ${y.std():,.2f}\")\nprint(f\"    - Range: ${y.min():,.2f} - ${y.max():,.2f}\")\n\n\n================================================================================\nFEATURE ENGINEERING FOR SALARY PREDICTION\n================================================================================\n\nCategorical features to encode (5):\n  ✓ STATE: 51 unique values\n  ✓ TITLE: 3286 unique values\n  ✓ political_leaning: 3 unique values\n  ✓ LIGHTCAST_SECTORS: 23 unique values\n  ✓ SOC_2: 1 unique values\n  ✓ MIN_YEARS_EXPERIENCE: numeric feature added\n  ✓ MAX_YEARS_EXPERIENCE: numeric feature added\n\n Total Features for Salary Prediction: 7\n\n Dataset Statistics:\n  • Feature Matrix Shape: (30808, 7)\n  • Salary Statistics:\n    - Mean: $117,953.76\n    - Median: $116,300.00\n    - Std Dev: $45,133.88\n    - Range: $15,860.00 - $500,000.00\n\n\nFeature Selection Justification: The features were selected based on their theoretical and empirical relationship with salary:\nSTATE & Political Leaning: Geographic location and political climate influence cost of living and compensation policies TITLE: Job title is the primary indicator of role level and responsibility LIGHTCAST_SECTORS: Industry sector determines baseline compensation structure SOC/NAICS: Occupation classification provides standardized job categorization Years of Experience: Direct correlation with salary progression (if available)\n\n\nShow code\n# Split data (70/30 as per assignment requirements)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42\n)\n\nprint(\"=\"*80)\nprint(\"TRAIN/TEST SPLIT\")\nprint(\"=\"*80)\nprint(f\"\\nTraining Set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Test Set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"\\nSalary Distribution:\")\nprint(f\"  Training - Mean: ${y_train.mean():,.2f}, Std: ${y_train.std():,.2f}\")\nprint(f\"  Test - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}\")\n\n\n================================================================================\nTRAIN/TEST SPLIT\n================================================================================\n\nTraining Set: 21,565 samples (70.0%)\nTest Set: 9,243 samples (30.0%)\n\nSalary Distribution:\n  Training - Mean: $117,747.63, Std: $45,170.79\n  Test - Mean: $118,434.66, Std: $45,046.43\n\n\n\n\nView Linear Regression code\nprint(\"=\"*80)\nprint(\"MODEL 1: MULTIPLE LINEAR REGRESSION\")\nprint(\"=\"*80)\n\n# Train model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Make predictions\ny_pred_lin = lin_reg.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))\nr2_lin = r2_score(y_test, y_pred_lin)\nmae_lin = np.mean(np.abs(y_test - y_pred_lin))\n\nprint(f\"\\n📊 Model Performance:\")\nprint(f\"   • R² Score: {r2_lin:.4f}\")\nprint(f\"     → Explains {r2_lin*100:.2f}% of salary variance\")\nprint(f\"   • RMSE: ${rmse_lin:,.2f}\")\nprint(f\"   • MAE: ${mae_lin:,.2f}\")\n\n# Feature coefficients\ncoef_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Coefficient': lin_reg.coef_\n}).sort_values('Coefficient', key=abs, ascending=False)\n\nprint(f\"\\n📈 Top 10 Most Influential Features:\")\nprint(coef_df.head(10).to_string(index=False))\n\n\n================================================================================\nMODEL 1: MULTIPLE LINEAR REGRESSION\n================================================================================\n\n📊 Model Performance:\n   • R² Score: 0.2106\n     → Explains 21.06% of salary variance\n   • RMSE: $40,021.31\n   • MAE: $31,035.39\n\n📈 Top 10 Most Influential Features:\n                  Feature   Coefficient\n     MIN_YEARS_EXPERIENCE  6.266993e+03\npolitical_leaning_encoded -4.017884e+03\n     MAX_YEARS_EXPERIENCE  2.308933e+03\nLIGHTCAST_SECTORS_encoded  4.382554e+02\n            STATE_encoded  3.158638e+01\n            TITLE_encoded  3.020147e+00\n            SOC_2_encoded  1.909939e-11\n\n\n\n\nShow code\n# Visualize top 15 feature coefficients\ntop_features = coef_df.head(15)\n\nfig = px.bar(\n    top_features,\n    x='Coefficient',\n    y='Feature',\n    orientation='h',\n    title='Top 15 Feature Coefficients - Multiple Linear Regression',\n    labels={'Coefficient': 'Impact on Salary ($)', 'Feature': 'Feature'},\n    color='Coefficient',\n    color_continuous_scale='RdBu',\n    color_continuous_midpoint=0\n)\n\nfig.update_layout(template=\"plotly_white\", height=500, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nView Random Forest code\nprint(\"=\"*80)\nprint(\"MODEL 2: RANDOM FOREST REGRESSION\")\nprint(\"=\"*80)\n\n# Train model\nrf_reg = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=20,\n    min_samples_split=10,\n    min_samples_leaf=4,\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"Training Random Forest model...\")\nrf_reg.fit(X_train, y_train)\nprint(\"✓ Training complete!\")\n\n# Make predictions\ny_pred_rf = rf_reg.predict(X_test)\n\n# Calculate evaluation metrics\nrmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nr2_rf = r2_score(y_test, y_pred_rf)\nmae_rf = np.mean(np.abs(y_test - y_pred_rf))\n\nprint(f\"\\n Model Performance:\")\nprint(f\"   • R² Score: {r2_rf:.4f}\")\nprint(f\"     → Explains {r2_rf*100:.2f}% of salary variance\")\nprint(f\"   • RMSE: ${rmse_rf:,.2f}\")\nprint(f\"   • MAE: ${mae_rf:,.2f}\")\n\n# Feature importance\nimportance_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Importance': rf_reg.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(f\"\\n Top 10 Most Important Features:\")\nprint(importance_df.head(10).to_string(index=False))\n\n# Calculate improvement safely\nif r2_lin &gt; 0:\n    improvement = ((r2_rf - r2_lin) / r2_lin) * 100\n    print(f\"\\n🚀 Random Forest improves R² by {improvement:.1f}% over Linear Regression\")\n\n\n================================================================================\nMODEL 2: RANDOM FOREST REGRESSION\n================================================================================\nTraining Random Forest model...\n✓ Training complete!\n\n Model Performance:\n   • R² Score: 0.5717\n     → Explains 57.17% of salary variance\n   • RMSE: $29,478.34\n   • MAE: $20,012.05\n\n Top 10 Most Important Features:\n                  Feature  Importance\n            TITLE_encoded    0.484861\n     MIN_YEARS_EXPERIENCE    0.346083\n            STATE_encoded    0.089213\nLIGHTCAST_SECTORS_encoded    0.048833\npolitical_leaning_encoded    0.019680\n     MAX_YEARS_EXPERIENCE    0.011329\n            SOC_2_encoded    0.000000\n\n🚀 Random Forest improves R² by 171.5% over Linear Regression\n\n\n\n\nShow code\n# Visualize feature importance\ntop_features_rf = importance_df.head(15)\n\nfig = px.bar(\n    top_features_rf,\n    x='Importance',\n    y='Feature',\n    orientation='h',\n    title='Top 15 Feature Importance - Random Forest Regression',\n    labels={'Importance': 'Importance Score', 'Feature': 'Feature'},\n    color='Importance',\n    color_continuous_scale='Viridis'\n)\n\nfig.update_layout(template=\"plotly_white\", height=500, font=dict(family=\"Roboto\", size=12))\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nShow code\nprint(\"=\"*80)\nprint(\"REGRESSION MODEL COMPARISON\")\nprint(\"=\"*80)\n\n# Create comparison dataframe\ncomparison_df = pd.DataFrame({\n    'Model': ['Multiple Linear Regression', 'Random Forest Regression'],\n    'R² Score': [r2_lin, r2_rf],\n    'RMSE ($)': [rmse_lin, rmse_rf],\n    'MAE ($)': [mae_lin, mae_rf]\n})\n\nprint(\"\\n\" + comparison_df.to_string(index=False))\n\nbest_model = comparison_df.loc[comparison_df['R² Score'].idxmax(), 'Model']\nbest_r2 = comparison_df['R² Score'].max()\n\nprint(f\"\\n🏆 Best Performing Model: {best_model}\")\nprint(f\"   • Achieves R² of {best_r2:.4f}\")\nprint(f\"   • Explains {best_r2*100:.2f}% of salary variance\")\n\n\n================================================================================\nREGRESSION MODEL COMPARISON\n================================================================================\n\n                     Model  R² Score     RMSE ($)      MAE ($)\nMultiple Linear Regression  0.210579 40021.308824 31035.389186\n  Random Forest Regression  0.571716 29478.338762 20012.049850\n\n🏆 Best Performing Model: Random Forest Regression\n   • Achieves R² of 0.5717\n   • Explains 57.17% of salary variance\n\n\n\n\nShow code\n# Sample for performance\nsample_size = min(2000, len(y_test))\nindices = np.random.choice(len(y_test), sample_size, replace=False)\n\ncomparison_results = pd.DataFrame({\n    'Actual': y_test.iloc[indices],\n    'Linear_Regression': y_pred_lin[indices],\n    'Random_Forest': y_pred_rf[indices]\n})\n\nfig = go.Figure()\n\n# Random Forest predictions\nfig.add_trace(go.Scatter(\n    x=comparison_results['Actual'],\n    y=comparison_results['Random_Forest'],\n    mode='markers',\n    name='Random Forest',\n    opacity=0.6,\n    marker=dict(size=5, color='blue')\n))\n\n# Linear Regression predictions\nfig.add_trace(go.Scatter(\n    x=comparison_results['Actual'],\n    y=comparison_results['Linear_Regression'],\n    mode='markers',\n    name='Linear Regression',\n    opacity=0.6,\n    marker=dict(size=5, color='red')\n))\n\n# Perfect prediction line\nmin_val = comparison_results['Actual'].min()\nmax_val = comparison_results['Actual'].max()\n\nfig.add_trace(go.Scatter(\n    x=[min_val, max_val],\n    y=[min_val, max_val],\n    mode='lines',\n    name='Perfect Prediction',\n    line=dict(color='green', dash='dash', width=2)\n))\n\nfig.update_layout(\n    title=f'Actual vs Predicted Salary - Model Comparison (n={sample_size:,})',\n    xaxis_title='Actual Salary ($)',\n    yaxis_title='Predicted Salary ($)',\n    template=\"plotly_white\",\n    height=550,\n    hovermode='closest'\n)\n\nfig.show()"
  },
  {
    "objectID": "data_cleaning.html#data-cleaning",
    "href": "data_cleaning.html#data-cleaning",
    "title": "Data Cleaning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe following code cleans and standardizes the job postings dataset — ensuring proper data types, filling missing salaries, removing duplicates, categorizing remote types, and dropping overly sparse columns — to produce a clean, analysis-ready DataFrame.\n\n\nCode\n# casting corrected variable type\njobs_df = jobs_df.withColumn(\"SALARY_FROM\", col (\"SALARY_FROM\").cast(\"float\"))\\\n  .withColumn(\"SALARY_TO\", col(\"SALARY_TO\").cast(\"float\")) \\\n  .withColumn(\"MAX_YEARS_EXPERIENCE\", col(\"MAX_YEARS_EXPERIENCE\").cast(\"float\"))\\\n  .withColumn(\"MIN_YEARS_EXPERIENCE\", col(\"MIN_YEARS_EXPERIENCE\").cast(\"float\"))\\\n  .withColumn(\"SALARY\", col(\"SALARY\").cast(\"float\"))\n\n# Clean Up Columns\njobs_df = jobs_df.withColumn(\"EDUCATION_LEVELS_NAME\", regexp_replace(col(\"EDUCATION_LEVELS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOURCE_TYPES\", regexp_replace(col(\"SOURCE_TYPES\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOURCES\", regexp_replace(col(\"SOURCES\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SKILLS\", regexp_replace(col(\"SKILLS\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SKILLS_NAME\", regexp_replace(col(\"SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SPECIALIZED_SKILLS_NAME\", regexp_replace(col(\"SPECIALIZED_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CERTIFICATIONS_NAME\", regexp_replace(col(\"CERTIFICATIONS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"COMMON_SKILLS_NAME\", regexp_replace(col(\"COMMON_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"SOFTWARE_SKILLS_NAME\", regexp_replace(col(\"SOFTWARE_SKILLS_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP6_NAME\", regexp_replace(col(\"CIP6_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP4_NAME\", regexp_replace(col(\"CIP4_NAME\"), \"[\\n\\r]\", \"\"))\njobs_df = jobs_df.withColumn(\"CIP2_NAME\", regexp_replace(col(\"CIP2_NAME\"), \"[\\n\\r]\", \"\"))\n\n\n# Compute and impute Median Salary\ndef compute_median(sdf, col_name):\n  q = sdf.approxQuantile(col_name, [0.5], 0.01)\n  return q[0] if q else None\n\n\nmedian_from = compute_median(jobs_df, \"SALARY_FROM\")\nmedian_to = compute_median(jobs_df, \"SALARY_TO\")\nmedian_salary = compute_median(jobs_df, \"SALARY\")\n\nprint(\"Medians:\", median_from, median_to, median_salary)\n\njobs_df = jobs_df.fillna({\n  \"SALARY_FROM\": median_from,\n  \"SALARY_TO\": median_to,\n  \"SALARY\": median_salary\n})\n\nfrom pyspark.sql.functions import col\njobs_df = jobs_df.withColumn(\n    \"MIDPOINT_SALARY\",\n    (col(\"SALARY_TO\") + col(\"SALARY_FROM\")) / 2\n)\n\n# Dropping unnecessary columns\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\"STATE\",\"COUNTY_OUTGOING\",\"COUNTY_INCOMMING\",\"MSA_OUTGOING\",\"MSA_INCOMING\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\", \"ONET\",\"ONET_2019\",\"CIP6\",\"CIP4\",\"CIP2\",\"SOC_2021_2\",\"SOC_2021_3\",\"SOC_2021_4\",\"SOC_2021_5\",\"SOC_2\", \"SOC_3\", \"SOC_4\",\"SOC_5\", \"NAICS_2022_2\",\"NAICS_2022_3\",\"NAICS_2022_4\",\"NAICS_2022_5\",\"NAICS_2022_6\",\"CITY\",\"COUNTY\",\"MSA\",\"COUNTY_INCOMING\"\n]\njobs_df = jobs_df.drop(*columns_to_drop)\n\n# configuring remote work groups\nfrom pyspark.sql.functions import when, col, trim\n\njobs_df = jobs_df.withColumn(\"REMOTE_GROUP\",\n  when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Remote\", \"Remote\")\n  .when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Hybrid Remote\", \"Hybrid\")\n  .when(trim(col(\"REMOTE_TYPE_NAME\"))== \"Not Remote\", \"Onsite\")\n  .when(col(\"REMOTE_TYPE_NAME\").isNull(), \"Onsite\")\n  .otherwise(\"Onsite\")\n)\n\n# dropping any duplicate postings\njobs_df = jobs_df.dropDuplicates([\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])\n\n# handling missing data\nfrom pyspark.sql.functions import col, when, sum as spark_sum\n\ntotal_rows = jobs_df.count()\nmissing_threshold = total_rows * 0.5\nnull_counts = jobs_df.select([\n    (spark_sum(col(c).isNull().cast(\"int\"))).alias(c) for c in jobs_df.columns\n]).collect()[0].asDict()\ncolumns_to_keep = [c for c, nulls in null_counts.items() if nulls &lt;= missing_threshold or c == \"SALARY\"]\njobs_df = jobs_df.select(columns_to_keep)\n\n#jobs_df.show(15)\n\n\nThis part of the script joins in another data frame that has the 2024 presidential election results by state. This allows us to use the states’ political affiliation as an attribute of the job posting.\n\n\nCode\nfrom pyspark.sql import functions as F\n\njobs_df = jobs_df.withColumn(\"STATE_ABBREVIATION\", F.trim(F.split(jobs_df[\"COUNTY_NAME\"], \",\").getItem(1)))\n\njobs_alias = jobs_df.alias(\"jobs\")\nelections_alias = elections_df.alias(\"elections\")\n\njobs_df = jobs_alias.join(\n    elections_alias,\n    F.col(\"jobs.STATE_ABBREVIATION\") == F.col(\"elections.STATE\"),\n    \"left\"\n)\njobs_df = jobs_df.drop(F.col(\"elections.STATE\"))\n\njobs_df = jobs_df.withColumnRenamed(\"Affiliation\", \"AFFILIATION\")\n\n#jobs_df.show(15)\n\n\nNow, this script selects only the columns we want to look at specifically\n\n\nCode\nselected_df = jobs_df.select(\n  \"EDUCATION_LEVELS_NAME\",\n  \"MIN_EDULEVELS_NAME\",\n  \"EMPLOYMENT_TYPE_NAME\",\n  \"MIN_YEARS_EXPERIENCE\",\n  \"SALARY_TO\",\n  \"SALARY_FROM\",\n  \"SALARY\",\n  \"CITY_NAME\",\n  \"MSA_NAME\",\n  \"STATE_NAME\",\n  \"NAICS2_NAME\",\n  \"NAICS3_NAME\",\n  \"NAICS4_NAME\",\n  \"NAICS5_NAME\",\n  \"NAICS6_NAME\",\n  \"SKILLS_NAME\",\n  \"SPECIALIZED_SKILLS_NAME\",\n  \"CERTIFICATIONS_NAME\",\n  \"COMMON_SKILLS_NAME\",\n  \"SOFTWARE_SKILLS_NAME\",\n  \"ONET_NAME\",\n  \"LOT_CAREER_AREA_NAME\",\n  \"LOT_OCCUPATION_NAME\",\n  \"LOT_SPECIALIZED_OCCUPATION_NAME\",\n  \"LOT_OCCUPATION_GROUP_NAME\",\n  \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n  \"LOT_V6_OCCUPATION_NAME\",\n  \"LOT_V6_OCCUPATION_GROUP_NAME\",\n  \"LOT_V6_CAREER_AREA_NAME\",\n  \"SOC_2_NAME\",\n  \"SOC_3_NAME\",\n  \"SOC_4_NAME\",\n  \"SOC_5_NAME\",\n  \"REMOTE_GROUP\",\n  \"STATE_ABBREVIATION\",\n  \"AFFILIATION\",\n  \"MIDPOINT_SALARY\"\n)\n\n\nOnce we have the columns we want to look at, we create a heat map to show us the remaining missing values. We have already dealt with a lot of missing values earlier, but this will help us visualize what is left.\n\n\nCode\nimport pandas as pd\nfrom pyspark.sql.functions import col, sum as spark_sum, when, trim, length\nimport hvplot.pandas\n\ndf_sample_viz = selected_df.select(\n  \"MIN_YEARS_EXPERIENCE\",\n  \"SALARY\",\n  \"MSA_NAME\",\n  \"NAICS5_NAME\"\n)\n\ndf_sample = df_sample_viz.sample(fraction = .15, seed = 42).toPandas()\n\nmissing_mask = df_sample.isnull()\n\nmissing_long = (\n  missing_mask.reset_index()\n  .melt(id_vars = \"index\", var_name = \"column\", value_name = \"is_missing\")\n)\n\nmissing_long[\"is_missing\"] = missing_long[\"is_missing\"].astype(int)\n\nmissing = missing_long.hvplot.heatmap(\n  x=\"column\",\n  y=\"index\",\n  C = \"is_missing\",\n  cmap = \"Blues\",\n  width = 900,\n  height = 500,\n  title = \"Heatmap of Missing Values (15%)\"\n).opts(xrotation=45)\n\nhvplot.save(missing, './output/missing_heatmap.html')\n\n\n\nAs you can see above, the missing values are mainly in the columns for minimum years of experience, and MSA name. The following script cleans up some of the column values and replaces missing values with an appropriate substitute such as 0 or “unknown”.\n\n\nCode\nfrom pyspark.sql.functions import countDistinct\n\nselected_df.select([\n  countDistinct(c).alias(c+\"_nunique\")\n  for c in selected_df.columns\n]).show(truncate=False)\n\n# Education Levels\n\nselected_df = selected_df.withColumn(\n  \"EDUCATION_LEVELS_NAME\",\n    when(col(\"EDUCATION_LEVELS_NAME\").isNull(), \"No Education Listed\")\n    .otherwise(col(\"EDUCATION_LEVELS_NAME\"))\n)\n\n# Min Edu Levels\n\nselected_df = selected_df.withColumn(\n  \"MIN_EDULEVELS_NAME\",\n    when(col(\"MIN_EDULEVELS_NAME\").isNull(), \"No Education Listed\")\n    .otherwise(col(\"MIN_EDULEVELS_NAME\"))\n)\n\n# Employment Type Name\n\nselected_df = selected_df.withColumn(\n  \"EMPLOYMENT_TYPE_NAME\",\n    when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Part-time / full-time\",\"Flexible\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Part-time (â‰¤ 32 hours)\",\"Part-Time\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\") == \"Full-time (&gt; 32 hours)\",\"Full-Time\")\n    .when(col(\"EMPLOYMENT_TYPE_NAME\").isNull(), \"Full-Time\")\n    .otherwise(col(\"EMPLOYMENT_TYPE_NAME\"))\n)\n\n# Min Years Experience\nselected_df = selected_df.withColumn(\n    \"MIN_YEARS_EXPERIENCE\",\n    when(col(\"MIN_YEARS_EXPERIENCE\").isNull(), 0)\n    .otherwise(col(\"MIN_YEARS_EXPERIENCE\"))\n)\n\n# Salary to\nselected_df = selected_df.withColumn(\n    \"SALARY_TO\",\n    when(col(\"SALARY_TO\").isNull(), median_to)\n    .otherwise(col(\"SALARY_TO\"))\n)\n\n# Salary from\nselected_df = selected_df.withColumn(\n    \"SALARY_FROM\",\n    when(col(\"SALARY_FROM\").isNull(), median_from)\n    .otherwise(col(\"SALARY_FROM\"))\n)\n\n# Salary \nselected_df = selected_df.withColumn(\n    \"SALARY\",\n    when(col(\"SALARY\").isNull(), median_salary)\n    .otherwise(col(\"SALARY\"))\n)\n\n# City Name\nselected_df = selected_df.withColumn(\n  \"CITY_NAME\",\n    when(col(\"CITY_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"CITY_NAME\"))\n)\n\n# MSA Name\nselected_df = selected_df.withColumn(\n  \"MSA_NAME\",\n    when(col(\"MSA_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"MSA_NAME\"))\n)\n\n# State Name\nselected_df = selected_df.withColumn(\n  \"STATE_NAME\",\n    when(col(\"STATE_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"STATE_NAME\"))\n)\n\n# NAICS2_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS2_NAME\",\n    when(col(\"NAICS2_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS2_NAME\"))\n)\n\n# NAICS3_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS3_NAME\",\n    when(col(\"NAICS3_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS3_NAME\"))\n)\n\n# NAICS4_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS4_NAME\",\n    when(col(\"NAICS4_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS4_NAME\"))\n)\n\n# NAICS5_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS5_NAME\",\n    when(col(\"NAICS5_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS5_NAME\"))\n)\n\n# NAICS6_NAME \nselected_df = selected_df.withColumn(\n  \"NAICS6_NAME\",\n    when(col(\"NAICS6_NAME\").isNull(), \"Unknown\")\n    .otherwise(col(\"NAICS6_NAME\"))\n)\n\n#STATE ABBREVIATION\nselected_df = selected_df.withColumn(\n  \"STATE_ABBREVIATION\",\n    when(col(\"STATE_ABBREVIATION\").isNull(), \"Unknown\")\n    .otherwise(col(\"STATE_ABBREVIATION\"))\n)\n\n\nFinally, we have a clean dataset so we will convert it to a pandas dataframe and save it a csv.\n\n\nCode\npdf = selected_df.toPandas()\n\npdf.to_csv(\"./data/lightcast_cleaned.csv\", index=False)\n\npdf.head(15)\n\nprint(\"Data Cleaning Complete. Rows retained:\", len(pdf))"
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The following visualizations are based on the lightcast job postings data frame that was cleaned in the previous section. This analysis explores different facets of the data specifically related to the political affiliation of the states and the different job postings in each state. We also take a closer look at AI related jobs and the impact of political climate on salary."
>>>>>>> a6d8aa94b2f68134495b0879e4e29169daf83bf2
  }
]